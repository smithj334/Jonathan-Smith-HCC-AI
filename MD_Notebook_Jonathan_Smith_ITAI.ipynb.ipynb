{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IhIyQBn2MY_"
      },
      "source": [
        "# Creating Numbers/images with AI: A Hands-on Diffusion Model Exercise\n",
        "\n",
        "## Introduction\n",
        "In this assignment, you'll learn how to create an AI model that can generate realistic images from scratch using a powerful technique called 'diffusion'. Think of it like teaching AI to draw by first learning how images get blurry and then learning to make them clear again.\n",
        "\n",
        "### What We'll Build\n",
        "- A diffusion model capable of generating realistic images\n",
        "- For most students: An AI that generates handwritten digits (0-9) using the MNIST dataset\n",
        "- For students with more computational resources: Options to work with more complex datasets\n",
        "- Visual demonstrations of how random noise gradually transforms into clear, recognizable images\n",
        "- By the end, your AI should create images realistic enough for another AI to recognize them\n",
        "\n",
        "### Dataset Options\n",
        "This lab offers flexibility based on your available computational resources:\n",
        "\n",
        "- Standard Option (Free Colab): We'll primarily use the MNIST handwritten digit dataset, which works well with limited GPU memory and completes training in a reasonable time frame. Most examples and code in this notebook are optimized for MNIST.\n",
        "\n",
        "- Advanced Option: If you have access to more powerful GPUs (either through Colab Pro/Pro+ or your own hardware), you can experiment with more complex datasets like Fashion-MNIST, CIFAR-10, or even face generation. You'll need to adapt the model architecture, hyperparameters, and evaluation metrics accordingly.\n",
        "\n",
        "### Resource Requirements\n",
        "- Basic MNIST: Works with free Colab GPUs (2-4GB VRAM), ~30 minutes training\n",
        "- Fashion-MNIST: Similar requirements to MNIST\n",
        "CIFAR-10: Requires more memory (8-12GB VRAM) and longer training (~2 hours)\n",
        "- Higher resolution images: Requires substantial GPU resources and several hours of training\n",
        "\n",
        "### Before You Start\n",
        "1. Make sure you're running this in Google Colab or another environment with GPU access\n",
        "2. Go to 'Runtime' → 'Change runtime type' and select 'GPU' as your hardware accelerator\n",
        "3. Each code cell has comments explaining what it does\n",
        "4. Don't worry if you don't understand every detail - focus on the big picture!\n",
        "5. If working with larger datasets, monitor your GPU memory usage carefully\n",
        "\n",
        "The concepts you learn with MNIST will scale to more complex datasets, so even if you're using the basic option, you'll gain valuable knowledge about generative AI that applies to more advanced applications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAD_17ST9_xE"
      },
      "source": [
        "## Step 1: Setting Up Our Tools\n",
        "First, let's install and import all the tools we need. Run this cell and wait for it to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KdNPBZ9q2UFs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7efad2-6751-496a-b0b2-9d6da9e1ad54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Package installation complete.\n",
            "We'll be using: cuda\n",
            "GPU name: Tesla T4\n",
            "GPU memory: 15.83 GB\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install required packages\n",
        "%pip install einops\n",
        "print(\"Package installation complete.\")\n",
        "\n",
        "# Step 2: Import libraries\n",
        "# --- Core PyTorch libraries ---\n",
        "import torch  # Main deep learning framework\n",
        "import torch.nn.functional as F  # Neural network functions like activation functions\n",
        "import torch.nn as nn  # Neural network building blocks (layers)\n",
        "from torch.optim import Adam  # Optimization algorithm for training\n",
        "\n",
        "# --- Data handling ---\n",
        "from torch.utils.data import Dataset, DataLoader  # For organizing and loading our data\n",
        "import torchvision  # Library for computer vision datasets and models\n",
        "import torchvision.transforms as transforms  # For preprocessing images\n",
        "\n",
        "# --- Tensor manipulation ---\n",
        "import random  # For random operations\n",
        "from einops.layers.torch import Rearrange  # For reshaping tensors in neural networks\n",
        "from einops import rearrange  # For elegant tensor reshaping operations\n",
        "import numpy as np  # For numerical operations on arrays\n",
        "\n",
        "# --- System utilities ---\n",
        "import os  # For operating system interactions (used for CPU count)\n",
        "\n",
        "# --- Visualization tools ---\n",
        "import matplotlib.pyplot as plt  # For plotting images and graphs\n",
        "from PIL import Image  # For image processing\n",
        "from torchvision.utils import save_image, make_grid  # For saving and displaying image grids\n",
        "\n",
        "# Step 3: Set up device (GPU or CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"We'll be using: {device}\")\n",
        "\n",
        "# Check if we're actually using GPU (for students to verify)\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"Note: Training will be much slower on CPU. Consider using Google Colab with GPU enabled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MheKNRUYCnP_"
      },
      "source": [
        "\n",
        "###  REPRODUCIBILITY AND DEVICE SETUP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eqahRmrvCnP_",
        "outputId": "74897b78-1ad4-4172-9786-31729ccec7ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random seeds set to 42 for reproducible results\n",
            "Available GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "# Step 4: Set random seeds for reproducibility\n",
        "# Diffusion models are sensitive to initialization, so reproducible results help with debugging\n",
        "SEED = 42  # Universal seed value for reproducibility\n",
        "torch.manual_seed(SEED)          # PyTorch random number generator\n",
        "np.random.seed(SEED)             # NumPy random number generator\n",
        "random.seed(SEED)                # Python's built-in random number generator\n",
        "\n",
        "print(f\"Random seeds set to {SEED} for reproducible results\")\n",
        "\n",
        "# Configure CUDA for GPU operations if available\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(SEED)       # GPU random number generator\n",
        "    torch.cuda.manual_seed_all(SEED)   # All GPUs random number generator\n",
        "\n",
        "    # Ensure deterministic GPU operations\n",
        "    # Note: This slightly reduces performance but ensures results are reproducible\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    try:\n",
        "        # Check available GPU memory\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # Convert to GB\n",
        "        print(f\"Available GPU Memory: {gpu_memory:.1f} GB\")\n",
        "\n",
        "        # Add recommendation based on memory\n",
        "        if gpu_memory < 4:\n",
        "            print(\"Warning: Low GPU memory. Consider reducing batch size if you encounter OOM errors.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not check GPU memory: {e}\")\n",
        "else:\n",
        "    print(\"No GPU detected. Training will be much slower on CPU.\")\n",
        "    print(\"If you're using Colab, go to Runtime > Change runtime type and select GPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0svL6Ik6Q1J"
      },
      "source": [
        "## Step 2: Choosing Your Dataset\n",
        "\n",
        "You have several options for this exercise, depending on your computer's capabilities:\n",
        "\n",
        "### Option 1: MNIST (Basic - Works on Free Colab)\n",
        "- Content: Handwritten digits (0-9)\n",
        "- Image size: 28x28 pixels, Grayscale\n",
        "- Training samples: 60,000\n",
        "- Memory needed: ~2GB GPU\n",
        "es on Colab\n",
        "- **Choose this if**: You're using free Colab or have a basic GPU\n",
        "\n",
        "### Option 2: Fashion-MNIST (Intermediate)\n",
        "- Content: Clothing items (shirts, shoes, etc.)\n",
        "- Image size: 28x28 pixels, Grayscale\n",
        "- Training samples: 60,000\n",
        "- Memory needed: ~2GB GPU\n",
        "- Training time: ~15-30 minutes on Colab\n",
        "- **Choose this if**: You want more interesting images but have limited GPU\n",
        "\n",
        "### Option 3: CIFAR-10 (Advanced)\n",
        "- Content: Real-world objects (cars, animals, etc.)\n",
        "- Image size: 32x32 pixels, Color (RGB)\n",
        "- Training samples: 50,000\n",
        "- Memory needed: ~4GB GPU\n",
        "- Training time: ~1-2 hours on Colab\n",
        "- **Choose this if**: You have Colab Pro or a good local GPU (8GB+ memory)\n",
        "\n",
        "### Option 4: CelebA (Expert)\n",
        "- Content: Celebrity face images\n",
        "- Image size: 64x64 pixels, Color (RGB)\n",
        "- Training samples: 200,000\n",
        "- Memory needed: ~8GB GPU\n",
        "- Training time: ~3-4 hours on Colab\n",
        "- **Choose this if**: You have excellent GPU (12GB+ memory)\n",
        "\n",
        "To use your chosen dataset, uncomment its section in the code below and make sure all others are commented out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "J1Jie0jRCnQA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "23cb7140-65ce-4ab7-96e8-95acd1f5c665"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIMG_SIZE = 32\\nIMG_CH = 3\\nN_CLASSES = 10\\nBATCH_SIZE = 32  # Reduced batch size for memory\\nEPOCHS = 50      # More epochs for complex data\\n\\n# Your code to create the transform and load CIFAR-10\\n# Hint: Use transforms.Normalize with RGB means and stds ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n# Then load torchvision.datasets.CIFAR10\\n\\n# Enter your code here:\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#===============================================================================\n",
        "# SECTION 2: DATASET SELECTION AND CONFIGURATION\n",
        "#===============================================================================\n",
        "# STUDENT INSTRUCTIONS:\n",
        "# 1. Choose ONE dataset option based on your available GPU memory\n",
        "# 2. Uncomment ONLY ONE dataset section below\n",
        "# 3. Make sure all other dataset sections remain commented out\n",
        "\n",
        "#-------------------------------------------\n",
        "# OPTION 1: MNIST (Basic - 2GB GPU)\n",
        "#-------------------------------------------\n",
        "# Recommended for: Free Colab or basic GPU\n",
        "# Memory needed: ~2GB GPU\n",
        "# Training time: ~15-30 minutes\n",
        "\n",
        "IMG_SIZE = 28\n",
        "IMG_CH = 1\n",
        "N_CLASSES = 10\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Your code to load the MNIST dataset\n",
        "# Hint: Use torchvision.datasets.MNIST with root='./data', train=True,\n",
        "#       transform=transform, and download=True\n",
        "# Then print a success message\n",
        "\n",
        "# Enter your code here:\n",
        "\n",
        "\n",
        "#-------------------------------------------\n",
        "# OPTION 2: Fashion-MNIST (Intermediate - 2GB GPU)\n",
        "#-------------------------------------------\n",
        "# Uncomment this section to use Fashion-MNIST instead\n",
        "\"\"\"\n",
        "IMG_SIZE = 28\n",
        "IMG_CH = 1\n",
        "N_CLASSES = 10\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Your code to load the Fashion-MNIST dataset\n",
        "# Hint: Very similar to MNIST but use torchvision.datasets.FashionMNIST\n",
        "\n",
        "# Enter your code here:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#-------------------------------------------\n",
        "# OPTION 3: CIFAR-10 (Advanced - 4GB+ GPU)\n",
        "#-------------------------------------------\n",
        "# Uncomment this section to use CIFAR-10 instead\n",
        "\"\"\"\n",
        "IMG_SIZE = 32\n",
        "IMG_CH = 3\n",
        "N_CLASSES = 10\n",
        "BATCH_SIZE = 32  # Reduced batch size for memory\n",
        "EPOCHS = 50      # More epochs for complex data\n",
        "\n",
        "# Your code to create the transform and load CIFAR-10\n",
        "# Hint: Use transforms.Normalize with RGB means and stds ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# Then load torchvision.datasets.CIFAR10\n",
        "\n",
        "# Enter your code here:\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================================\n",
        "# SECTION 2: DATASET SELECTION AND CONFIGURATION\n",
        "#===============================================================================\n",
        "# STUDENT INSTRUCTIONS:\n",
        "# 1. Choose ONE dataset option based on your available GPU memory\n",
        "# 2. Uncomment ONLY ONE dataset section below\n",
        "# 3. Make sure all other dataset sections remain commented out\n",
        "\n",
        "#-------------------------------------------\n",
        "# OPTION 1: MNIST (Basic - 2GB GPU)\n",
        "#-------------------------------------------\n",
        "# Recommended for: Free Colab or basic GPU\n",
        "# Memory needed: ~2GB GPU\n",
        "# Training time: ~15-30 minutes\n",
        "\n",
        "IMG_SIZE = 28\n",
        "IMG_CH = 1\n",
        "N_CLASSES = 10\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Your code to load the MNIST dataset\n",
        "# Hint: Use torchvision.datasets.MNIST with root='./data', train=True,\n",
        "#       transform=transform, and download=True\n",
        "# Then print a success message\n",
        "\n",
        "# Enter your code here:\n",
        "# (This assumes you have already run:\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "# )\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root='./data',\n",
        "    train=True,\n",
        "    transform=transform,\n",
        "    download=True\n",
        ")\n",
        "\n",
        "print(\"✅ Successfully loaded MNIST training dataset.\")\n",
        "print(f\"   - Dataset size: {len(train_dataset)} samples\")\n",
        "print(f\"   - Image config: {IMG_SIZE}x{IMG_SIZE}x{IMG_CH}\")\n",
        "\n",
        "\n",
        "#-------------------------------------------\n",
        "# OPTION 2: Fashion-MNIST (Intermediate - 2GB GPU)\n",
        "#-------------------------------------------\n",
        "# Uncomment this section to use Fashion-MNIST instead\n",
        "\"\"\"\n",
        "IMG_SIZE = 28\n",
        "IMG_CH = 1\n",
        "N_CLASSES = 10\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Your code to load the Fashion-MNIST dataset\n",
        "# Hint: Very similar to MNIST but use torchvision.datasets.FashionMNIST\n",
        "\n",
        "# Enter your code here:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "#-------------------------------------------\n",
        "# OPTION 3: CIFAR-10 (Advanced - 4GB+ GPU)\n",
        "#-------------------------------------------\n",
        "# Uncomment this section to use CIFAR-10 instead\n",
        "\"\"\"\n",
        "IMG_SIZE = 32\n",
        "IMG_CH = 3\n",
        "N_CLASSES = 10\n",
        "BATCH_SIZE = 32  # Reduced batch size for memory\n",
        "EPOCHS = 50      # More epochs for complex data\n",
        "\n",
        "# Your code to create the transform and load CIFAR-10\n",
        "# Hint: Use transforms.Normalize with RGB means and stds ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "# Then load torchvision.datasets.CIFAR10\n",
        "\n",
        "# Enter your code here:\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "M6tiyNCKFTR1",
        "outputId": "c74c4a19-6e38-4b83-8895-d449b9e26275"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:02<00:00, 4.79MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 132kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 13.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Successfully loaded MNIST training dataset.\n",
            "   - Dataset size: 60000 samples\n",
            "   - Image config: 28x28x1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIMG_SIZE = 32\\nIMG_CH = 3\\nN_CLASSES = 10\\nBATCH_SIZE = 32  # Reduced batch size for memory\\nEPOCHS = 50      # More epochs for complex data\\n\\n# Your code to create the transform and load CIFAR-10\\n# Hint: Use transforms.Normalize with RGB means and stds ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\\n# Then load torchvision.datasets.CIFAR10\\n\\n# Enter your code here:\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Validating Dataset Selection\n",
        "#Let's add code to validate that a dataset was selected\n",
        "# and check if your GPU has enough memory:\n",
        "\n",
        "# Validate dataset selection\n",
        "# *** I've changed 'dataset' to 'train_dataset' to match your variable ***\n",
        "if 'train_dataset' not in locals():\n",
        "    raise ValueError(\"\"\"\n",
        "    ❌ ERROR: No dataset selected! Please uncomment exactly one dataset option.\n",
        "    (Note: I'm checking for 'train_dataset', which you loaded successfully.)\n",
        "    Available options:\n",
        "    1. MNIST (Basic) - 2GB GPU\n",
        "    2. Fashion-MNIST (Intermediate) - 2GB GPU\n",
        "    3. CIFAR-10 (Advanced) - 4GB+ GPU\n",
        "    4. CelebA (Expert) - 8GB+ GPU\n",
        "    \"\"\")\n",
        "else:\n",
        "    print(\"✅ Dataset variable 'train_dataset' found.\")\n",
        "\n",
        "\n",
        "# Your code to validate GPU memory requirements\n",
        "# Hint: Check torch.cuda.is_available() and use torch.cuda.get_device_properties(0).total_memory\n",
        "# to get available GPU memory, then compare with dataset requirements\n",
        "\n",
        "# Enter your code here:\n",
        "import torch # Added import just in case this is a new cell\n",
        "\n",
        "# 1. Define memory requirements based on the loaded dataset's variables\n",
        "required_gb = 0\n",
        "dataset_name = \"Unknown\"\n",
        "\n",
        "# Use the variables set in the previous cell to determine requirements\n",
        "if 'IMG_SIZE' in locals() and IMG_SIZE == 28 and IMG_CH == 1:\n",
        "    required_gb = 2\n",
        "    dataset_name = \"MNIST / Fashion-MNIST\"\n",
        "elif 'IMG_SIZE' in locals() and IMG_SIZE == 32 and IMG_CH == 3:\n",
        "    required_gb = 4\n",
        "    dataset_name = \"CIFAR-10\"\n",
        "# (You could add more elifs here for other datasets like CelebA)\n",
        "\n",
        "print(f\"ℹ️  Selected dataset ({dataset_name}) requires ~{required_gb} GB of GPU memory.\")\n",
        "\n",
        "# 2. Check available GPU memory\n",
        "if torch.cuda.is_available():\n",
        "    # Get properties of the current GPU (device 0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    # Convert total memory from bytes to GiB (1024^3)\n",
        "    total_memory_gb = props.total_memory / (1024**3)\n",
        "\n",
        "    print(f\"✅ GPU found: {props.name}\")\n",
        "    print(f\"   - Total Memory: {total_memory_gb:.2f} GB\")\n",
        "\n",
        "    # 3. Compare and validate\n",
        "    if total_memory_gb < required_gb:\n",
        "        raise ValueError(f\"\"\"\n",
        "        ❌ ERROR: GPU memory insufficient for {dataset_name}!\n",
        "        - Required: ~{required_gb} GB\n",
        "        - Available: {total_memory_gb:.2f} GB\n",
        "        Please restart the runtime, select a smaller dataset (e.g., MNIST),\n",
        "        or get a session with a more powerful GPU.\n",
        "        \"\"\")\n",
        "    else:\n",
        "        print(f\"✅ GPU memory is sufficient.\")\n",
        "\n",
        "else:\n",
        "    # Warning if no GPU is found, as training will be very slow\n",
        "    print(\"⚠️ WARNING: No GPU found (torch.cuda.is_available() is False).\")\n",
        "    print(\"   Training will run on the CPU, which will be extremely slow.\")\n",
        "    if required_gb > 2:\n",
        "         print(f\"   This dataset ({dataset_name}) is not recommended for CPU-only training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wsvd3BI8GZKB",
        "outputId": "182c5fee-cc4b-464e-99bd-c38262f271cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Dataset variable 'train_dataset' found.\n",
            "ℹ️  Selected dataset (MNIST / Fashion-MNIST) requires ~2 GB of GPU memory.\n",
            "✅ GPU found: Tesla T4\n",
            "   - Total Memory: 14.74 GB\n",
            "✅ GPU memory is sufficient.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset Properties and Data Loaders\n",
        "#Now let's examine our dataset\n",
        "#and set up the data loaders:\n",
        "\n",
        "# (Importing necessary libraries)\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Your code to check sample batch properties\n",
        "# Hint: Get a sample batch using next(iter(DataLoader(dataset, batch_size=1)))\n",
        "# Then print information about the dataset shape, type, and value ranges\n",
        "\n",
        "# Enter your code here:\n",
        "print(\"--- 1. Dataset Sample Check ---\")\n",
        "# Create a temporary loader to grab one sample\n",
        "temp_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "sample_image, sample_label = next(iter(temp_loader))\n",
        "\n",
        "print(f\"Sample image batch shape: {sample_image.shape}\")\n",
        "print(f\"Sample image data type: {sample_image.dtype}\")\n",
        "print(f\"Sample image value range: Min={sample_image.min():.2f}, Max={sample_image.max():.2f}\")\n",
        "print(f\"Sample label: {sample_label.item()}\")\n",
        "print(\"Note: A range of -1.0 to 1.0 confirms Normalize((0.5,), (0.5,)) worked.\")\n",
        "del temp_loader, sample_image, sample_label # Clean up temp variables\n",
        "\n",
        "\n",
        "#===============================================================================\n",
        "# SECTION 3: DATASET SPLITTING AND DATALOADER CONFIGURATION\n",
        "#===============================================================================\n",
        "# Create train-validation split\n",
        "\n",
        "# Your code to create a train-validation split (80% train, 20% validation)\n",
        "# Hint: Use random_split() with appropriate train_size and val_size\n",
        "# Be sure to use a fixed generator for reproducibility\n",
        "\n",
        "# Enter your code here:\n",
        "print(\"\\n--- 2. Train/Validation Split ---\")\n",
        "dataset_size = len(train_dataset)\n",
        "val_size = int(dataset_size * 0.2)  # 20% for validation\n",
        "train_size = dataset_size - val_size # 80% for training\n",
        "\n",
        "# Use a fixed generator for reproducible splits\n",
        "generator = torch.Generator().manual_seed(42)\n",
        "train_set, val_set = random_split(\n",
        "    train_dataset,\n",
        "    [train_size, val_size],\n",
        "    generator=generator\n",
        ")\n",
        "\n",
        "print(f\"Original dataset size: {dataset_size}\")\n",
        "print(f\"Training set size:   {len(train_set)}\")\n",
        "print(f\"Validation set size: {len(val_set)}\")\n",
        "\n",
        "\n",
        "# Your code to create dataloaders for training and validation\n",
        "# Hint: Use DataLoader with batch_size=BATCH_SIZE, appropriate shuffle settings,\n",
        "# and num_workers based on available CPU cores\n",
        "\n",
        "# Enter your code here:\n",
        "print(\"\\n--- 3. DataLoaders Configuration ---\")\n",
        "# Use all available CPU cores for loading, or a reasonable number (e.g., 2)\n",
        "# os.cpu_count() is a good default\n",
        "num_workers = min(os.cpu_count(), 8) # Cap at 8 workers to be safe\n",
        "print(f\"Using {num_workers} workers for data loading.\")\n",
        "\n",
        "# BATCH_SIZE was defined in the previous cell (it should be 64 for MNIST)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # Shuffle training data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True # Speeds up data transfer to GPU\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_set,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False, # No need to shuffle validation data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"✅ Successfully created train_loader and val_loader.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikSElxMZGvRw",
        "outputId": "e20be03e-e3cf-4fe3-ff5b-053cce24a687"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- 1. Dataset Sample Check ---\n",
            "Sample image batch shape: torch.Size([1, 1, 28, 28])\n",
            "Sample image data type: torch.float32\n",
            "Sample image value range: Min=-1.00, Max=0.99\n",
            "Sample label: 1\n",
            "Note: A range of -1.0 to 1.0 confirms Normalize((0.5,), (0.5,)) worked.\n",
            "\n",
            "--- 2. Train/Validation Split ---\n",
            "Original dataset size: 60000\n",
            "Training set size:   48000\n",
            "Validation set size: 12000\n",
            "\n",
            "--- 3. DataLoaders Configuration ---\n",
            "Using 2 workers for data loading.\n",
            "✅ Successfully created train_loader and val_loader.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpzBxBj-2bLw"
      },
      "source": [
        "## Step 3: Building Our Model Components\n",
        "\n",
        "Now we'll create the building blocks of our AI model. Think of these like LEGO pieces that we'll put together to make our number generator:\n",
        "\n",
        "- GELUConvBlock: The basic building block that processes images\n",
        "- DownBlock: Makes images smaller while finding important features\n",
        "- UpBlock: Makes images bigger again while keeping the important features\n",
        "- Other blocks: Help the model understand time and what number to generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XyM8_DxgCnQC"
      },
      "outputs": [],
      "source": [
        "# Basic building block that processes images\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        \"\"\"\n",
        "        Creates a block with convolution, normalization, and activation\n",
        "\n",
        "        Args:\n",
        "            in_ch (int): Number of input channels\n",
        "            out_ch (int): Number of output channels\n",
        "            group_size (int): Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Check that group_size is compatible with out_ch\n",
        "        if out_ch % group_size != 0:\n",
        "            print(f\"Warning: out_ch ({out_ch}) is not divisible by group_size ({group_size})\")\n",
        "            # Adjust group_size to be compatible\n",
        "            group_size = min(group_size, out_ch)\n",
        "            while out_ch % group_size != 0:\n",
        "                group_size -= 1\n",
        "            print(f\"Adjusted group_size to {group_size}\")\n",
        "\n",
        "        # Your code to create layers for the block\n",
        "        # Hint: Use nn.Conv2d, nn.GroupNorm, and nn.GELU activation\n",
        "        # Then combine them using nn.Sequential\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code for the forward pass\n",
        "        # Hint: Simply pass the input through the model\n",
        "\n",
        "        # Enter your code here:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes you have already run: import torch.nn as nn)\n",
        "\n",
        "# Basic building block that processes images\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        \"\"\"\n",
        "        Creates a block with convolution, normalization, and activation\n",
        "\n",
        "        Args:\n",
        "            in_ch (int): Number of input channels\n",
        "            out_ch (int): Number of output channels\n",
        "            group_size (int): Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Check that group_size is compatible with out_ch\n",
        "        if out_ch % group_size != 0:\n",
        "            print(f\"Warning: out_ch ({out_ch}) is not divisible by group_size ({group_size})\")\n",
        "            # Adjust group_size to be compatible\n",
        "            group_size = min(group_size, out_ch)\n",
        "            while out_ch % group_size != 0:\n",
        "                group_size -= 1\n",
        "            print(f\"Adjusted group_size to {group_size}\")\n",
        "\n",
        "        # Your code to create layers for the block\n",
        "        # Hint: Use nn.Conv2d, nn.GroupNorm, and nn.GELU activation\n",
        "        # Then combine them using nn.Sequential\n",
        "\n",
        "        # Enter your code here:\n",
        "        self.model = nn.Sequential(\n",
        "            # 3x3 convolution with padding=1 to keep image size the same\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            # Group normalization\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            # GELU activation\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code for the forward pass\n",
        "        # Hint: Simply pass the input through the model\n",
        "\n",
        "        # Enter your_code here:\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "6fckXhuXHCr6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AZGtBKgMCnQD"
      },
      "outputs": [],
      "source": [
        "# Rearranges pixels to downsample the image (2x reduction in spatial dimensions)\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        \"\"\"\n",
        "        Downsamples the spatial dimensions by 2x while preserving information\n",
        "\n",
        "        Args:\n",
        "            in_chs (int): Number of input channels\n",
        "            group_size (int): Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Your code to create the rearrange operation and convolution\n",
        "        # Hint: Use Rearrange from einops.layers.torch to reshape pixels\n",
        "        # Then add a GELUConvBlock to process the rearranged tensor\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code for the forward pass\n",
        "        # Hint: Apply rearrange to downsample, then apply convolution\n",
        "\n",
        "        # Enter your code here:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes you have already run:\n",
        "# from einops.layers.torch import Rearrange\n",
        "# )\n",
        "\n",
        "# Rearranges pixels to downsample the image (2x reduction in spatial dimensions)\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        \"\"\"\n",
        "        Downsamples the spatial dimensions by 2x while preserving information\n",
        "\n",
        "        Args:\n",
        "            in_chs (int): Number of input channels\n",
        "            group_size (int): Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Your code to create the rearrange operation and convolution\n",
        "        # Hint: Use Rearrange from einops.layers.torch to reshape pixels\n",
        "        # Then add a GELUConvBlock to process the rearranged tensor\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # This operation takes 2x2 patches and moves them to the channel dimension\n",
        "        # 'b c (h 2) (w 2)' -> 'b (c 4) h w'\n",
        "        # This reduces H and W by 2, and increases C by 4\n",
        "        self.rearrange = Rearrange('b c (h 2) (w 2) -> b (c 4) h w')\n",
        "\n",
        "        # The number of input channels for the conv block is now 4 * in_chs\n",
        "        # We'll keep the output channels the same for this block\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        # We need to make sure the group_size is valid for the new channel count\n",
        "        if new_chs % group_size != 0:\n",
        "            # Adjust group_size to be a divisor of new_chs\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0:\n",
        "                valid_group_size -= 1\n",
        "            print(f\"RearrangePoolBlock adjusted group_size from {group_size} to {valid_group_size} for {new_chs} channels\")\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code for the forward pass\n",
        "        # Hint: Apply rearrange to downsample, then apply convolution\n",
        "\n",
        "        # Enter your code here:\n",
        "        # 1. Downsample by rearrangement\n",
        "        x = self.rearrange(x)\n",
        "        # 2. Process with convolution\n",
        "        x = self.conv_block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pbwN2RHgHVfj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes 'torch', 'torch.nn as nn', and 'GELUConvBlock' are defined)\n",
        "\n",
        "#Now let's implement the upsampling block for our U-Net architecture:\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block for decoding path in U-Net architecture.\n",
        "\n",
        "    This block:\n",
        "    1. Takes features from the decoding path and corresponding skip connection\n",
        "    2. Concatenates them along the channel dimension\n",
        "    3. Upsamples spatial dimensions by 2x using transposed convolution\n",
        "    4. Processes features through multiple convolutional blocks\n",
        "\n",
        "    Args:\n",
        "        in_chs (int): Number of input channels from the previous layer\n",
        "        out_chs (int): Number of output channels\n",
        "        group_size (int): Number of groups for GroupNorm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Your code to create the upsampling operation\n",
        "        # Hint: Use nn.ConvTranspose2d with kernel_size=2 and stride=2\n",
        "        # This layer upsamples the input 'x' from [B, in_chs, H, W]\n",
        "        # to match the skip connection's size: [B, in_chs, 2H, 2W]\n",
        "        # Enter your code here:\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "\n",
        "\n",
        "        # Your code to create the convolutional blocks\n",
        "        # Hint: Use multiple GELUConvBlocks in sequence\n",
        "        # After concatenation, channels will be (in_chs + in_chs) = 2 * in_chs\n",
        "        # These blocks process the concatenated features and output 'out_chs'\n",
        "        # Enter your code here:\n",
        "        self.convs = nn.Sequential(\n",
        "            # First block takes concatenated features and outputs 'out_chs'\n",
        "            GELUConvBlock(2 * in_chs, out_chs, group_size),\n",
        "            # Second block refines the features\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "\n",
        "        # Log the configuration for debugging\n",
        "        print(f\"Created UpBlock: in_chs={in_chs}, out_chs={out_chs}, spatial_increase=2x\")\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        \"\"\"\n",
        "        Forward pass through the UpBlock.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor from previous layer [B, in_chs, H, W]\n",
        "            skip (torch.Tensor): Skip connection tensor from encoder [B, in_chs, 2H, 2W]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor with shape [B, out_chs, 2H, 2W]\n",
        "        \"\"\"\n",
        "        # Your code for the forward pass\n",
        "        # Hint: Upsample x, then concatenate with skip, then process\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # 1. Upsample x to match skip's spatial dimensions\n",
        "        x_up = self.up(x)  # Shape: [B, in_chs, 2H, 2W]\n",
        "\n",
        "        # 2. Concatenate along the channel dimension (dim=1)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1) # Shape: [B, 2*in_chs, 2H, 2W]\n",
        "\n",
        "        # 3. Process with convolutional blocks\n",
        "        return self.convs(x_cat) # Shape: [B, out_chs, 2H, 2W]"
      ],
      "metadata": {
        "id": "ZZifTbvm5OkZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#===============================================================================\n",
        "# SECTION 4: DIFFUSION NOISE SCHEDULE\n",
        "#===============================================================================\n",
        "# We define the \"noise schedule\" - how much noise we add at each timestep.\n",
        "# This is a critical part of the diffusion model.\n",
        "\n",
        "# Number of steps in the diffusion process\n",
        "n_steps = 100 # This is the 'T' in the UNet\n",
        "beta_start = 1e-4\n",
        "beta_end = 0.02\n",
        "\n",
        "# 1. Create the 'beta' schedule (how much noise to add at step t)\n",
        "# We use a linear schedule for simplicity\n",
        "betas = torch.linspace(beta_start, beta_end, n_steps).to(device)\n",
        "\n",
        "# 2. Calculate 'alphas', which represent the 'signal rate' (1 - noise)\n",
        "alphas = 1.0 - betas\n",
        "\n",
        "# 3. Calculate 'alpha_bar' (cumulative product of alphas)\n",
        "# This tells us the total signal rate at step t\n",
        "alpha_bar = torch.cumprod(alphas, dim=0)\n",
        "\n",
        "# 4. Pre-calculate values for the forward 'add_noise' process\n",
        "# These are the terms in the formula: x_t = sqrt(a_bar) * x_0 + sqrt(1 - a_bar) * noise\n",
        "sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
        "sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - alpha_bar)\n",
        "\n",
        "# 5. Pre-calculate values for the reverse 'remove_noise' process\n",
        "# These are the terms needed for the DDPM sampling formula\n",
        "sqrt_recip_alpha = torch.sqrt(1.0 / alphas)\n",
        "sqrt_recip_alpha_bar = torch.sqrt(1.0 / alpha_bar)\n",
        "sqrt_recip_m1_alpha_bar = torch.sqrt(1.0 / alpha_bar - 1)\n",
        "posterior_variance = betas * (1.0 - torch.roll(alpha_bar, 1, 0)) / (1.0 - alpha_bar)\n",
        "posterior_variance[0] = betas[0] # Set first value (no previous alpha_bar)\n",
        "posterior_log_variance = torch.log(posterior_variance.clamp(min=1e-20))\n",
        "\n",
        "print(f\"✅ Noise schedule created with {n_steps} steps.\")\n",
        "print(f\"   - betas: {betas.shape}\")\n",
        "print(f\"   - alpha_bar: {alpha_bar.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_oR71r4NbwW",
        "outputId": "8736ec8d-9e09-4ffa-f0cb-dd9e9151e884"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Noise schedule created with 100 steps.\n",
            "   - betas: torch.Size([100])\n",
            "   - alpha_bar: torch.Size([100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# from torch.optim import Adam\n",
        "# model, device, IMG_SIZE, IMG_CH, n_steps, N_CLASSES,\n",
        "# train_loader, val_loader, UNet\n",
        "# )\n",
        "from torch.optim import Adam # Added import for the optimizer\n",
        "\n",
        "# Create our model and move it to GPU if available\n",
        "model = UNet(\n",
        "    T=n_steps,                 # Number of diffusion time steps\n",
        "    img_ch=IMG_CH,             # Number of channels in our images (1 for grayscale, 3 for RGB)\n",
        "    img_size=IMG_SIZE,         # Size of input images (28 for MNIST, 32 for CIFAR-10)\n",
        "    down_chs=(32, 64, 128),    # Channel dimensions for each downsampling level\n",
        "    t_embed_dim=8,             # Dimension for time step embeddings\n",
        "    c_embed_dim=N_CLASSES      # Number of classes for conditioning\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Input resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Input channels: {IMG_CH}\")\n",
        "print(f\"Time steps: {n_steps}\")\n",
        "print(f\"Condition classes: {N_CLASSES}\")\n",
        "print(f\"GPU acceleration: {'Yes' if device.type == 'cuda' else 'No'}\")\n",
        "\n",
        "# Validate model parameters and estimate memory requirements\n",
        "# Hint: Create functions to count parameters and estimate memory usage\n",
        "\n",
        "# Enter your code here:\n",
        "def count_parameters(model):\n",
        "    \"\"\"Counts the total number of trainable parameters in a model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total Trainable Parameters: {total_params:,} (~{total_params/1e6:.2f} M)\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    # Memory already allocated just for the model weights\n",
        "    allocated_mb = torch.cuda.memory_allocated(device) / (1024**2)\n",
        "    print(f\"Model VRAM (weights only): {allocated_mb:.2f} MB\")\n",
        "    print(\"Note: Total VRAM usage during training will be much higher due to gradients,\")\n",
        "    print(\"      optimizer states (Adam), and batch activations.\")\n",
        "\n",
        "\n",
        "# Your code to verify data ranges and integrity\n",
        "# Hint: Create functions to check data ranges in training and validation data\n",
        "\n",
        "# Enter your code here:\n",
        "def check_data_loader(loader, name):\n",
        "    \"\"\"Grabs one batch and prints its properties to check integrity.\"\"\"\n",
        "    print(f\"\\n--- Checking {name} ---\")\n",
        "    try:\n",
        "        # Get one batch and move it to the CPU for checking\n",
        "        images, labels = next(iter(loader))\n",
        "        images, labels = images.cpu(), labels.cpu()\n",
        "\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Image data type:   {images.dtype}\")\n",
        "        print(f\"  Image min/max/mean: {images.min():.2f} / {images.max():.2f} / {images.mean():.2f}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Label data type:   {labels.dtype}\")\n",
        "        print(f\"  Label min/max:     {labels.min()} / {labels.max()}\")\n",
        "        print(f\"  Image has NaNs:    {torch.isnan(images).any()}\")\n",
        "        print(f\"  Image has Infs:    {torch.isinf(images).any()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error checking {name}: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"DATA LOADER INTEGRITY CHECK\")\n",
        "print(f\"{'='*50}\")\n",
        "check_data_loader(train_loader, \"Training Loader\")\n",
        "check_data_loader(val_loader, \"Validation Loader\")\n",
        "print(\"\\nCheck: Image min/max should be approx. [-1.0, 1.0].\")\n",
        "print(\"Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\")\n",
        "\n",
        "\n",
        "# Set up the optimizer with parameters tuned for diffusion models\n",
        "# Note: Lower learning rates tend to work better for diffusion models\n",
        "initial_lr = 0.001  # Starting learning rate\n",
        "weight_decay = 1e-5  # L2 regularization to prevent overfitting\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=initial_lr,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
        "# This helps fine-tune the model toward the end of training\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',              # Reduce LR when monitored value stops decreasing\n",
        "    factor=0.5,              # Multiply LR by this factor\n",
        "    patience=5,              # Number of epochs with no improvement after which LR will be reduced\n",
        "    # verbose=True,          # <-- THIS LINE WAS REMOVED TO FIX THE TypeError\n",
        "    min_lr=1e-6              # Lower bound on the learning rate\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\")\n",
        "\n",
        "# STUDENT EXPERIMENT:\n",
        "# Try different channel configurations and see how they affect:\n",
        "# 1. Model size (parameter count)\n",
        "# 2. Training time\n",
        "# 3. Generated image quality\n",
        "#\n",
        "# Suggestions:\n",
        "# - Smaller: down_chs=(16, 32, 64)\n",
        "# - Larger: down_chs=(64, 128, 256, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJCqZy2sNuBy",
        "outputId": "7170b4ba-efb1-4204-a9ba-6fe82c949d80"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL ARCHITECTURE SUMMARY\n",
            "==================================================\n",
            "Input resolution: 28x28\n",
            "Input channels: 1\n",
            "Time steps: 100\n",
            "Condition classes: 10\n",
            "GPU acceleration: Yes\n",
            "Total Trainable Parameters: 3,230,412 (~3.23 M)\n",
            "Model VRAM (weights only): 12.33 MB\n",
            "Note: Total VRAM usage during training will be much higher due to gradients,\n",
            "      optimizer states (Adam), and batch activations.\n",
            "\n",
            "==================================================\n",
            "DATA LOADER INTEGRITY CHECK\n",
            "==================================================\n",
            "\n",
            "--- Checking Training Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.74\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "--- Checking Validation Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.73\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "Check: Image min/max should be approx. [-1.0, 1.0].\n",
            "Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\n",
            "\n",
            "✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        # Fix group_size if not divisible\n",
        "        if out_ch % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while out_ch % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if out_ch % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = 1\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        # Fix for EinopsError: Use named parameters p1=2, p2=2\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        # Fix group_size for new channel count\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "# 3. HELPER CLASS: DownBlock\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 4. HELPER CLASS: UpBlock\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            GELUConvBlock(2 * in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x_up = self.up(x)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.conv(x_cat)\n",
        "\n",
        "# 5. MAIN UNET CLASS (THIS IS THE MISSING ONE)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "        GS = 8 # Default Group Size\n",
        "        self.down_chs = down_chs\n",
        "        self.t_embed_dim = t_embed_dim\n",
        "        self.c_embed_dim = c_embed_dim\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Class embedding (assumes N_CLASSES is globally defined)\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.init_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Downsampling path\n",
        "        self.downs = nn.ModuleList()\n",
        "        for i in range(len(down_chs) - 1):\n",
        "            self.downs.append(\n",
        "                DownBlock(down_chs[i], down_chs[i+1], GS)\n",
        "            )\n",
        "\n",
        "        # Middle"
      ],
      "metadata": {
        "id": "IVzRtSH_15on"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        # Fix group_size if not divisible\n",
        "        if out_ch % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while out_ch % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if out_ch % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = 1\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        # Fix for EinopsError: Use named parameters p1=2, p2=2\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        # Fix group_size for new channel count\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "# 3. HELPER CLASS: DownBlock\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 4. HELPER CLASS: UpBlock\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            GELUConvBlock(2 * in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x_up = self.up(x)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.conv(x_cat)\n",
        "\n",
        "# 5. MAIN UNET CLASS (THIS IS THE MISSING ONE)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "        GS = 8 # Default Group Size\n",
        "        self.down_chs = down_chs\n",
        "        self.t_embed_dim = t_embed_dim\n",
        "        self.c_embed_dim = c_embed_dim\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Class embedding (assumes N_CLASSES is globally defined)\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.init_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Downsampling path\n",
        "        self.downs = nn.ModuleList()\n",
        "        for i in range(len(down_chs) - 1):\n",
        "            self.downs.append(\n",
        "                DownBlock(down_chs[i], down_chs[i+1], GS)\n",
        "            )\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mids = nn.Sequential(\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS),\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS)\n",
        "        )\n",
        "        self.mid_t_proj = nn.Linear(t_embed_dim, down_chs[-1])\n",
        "        self.mid_c_proj = nn.Linear(c_embed_dim, down_chs[-1])\n",
        "\n",
        "        # Upsampling path (Fixed IndexError)\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(len(down_chs)-1, 0, -1):\n",
        "            self.ups.append(\n",
        "                UpBlock(down_chs[i], down_chs[i-1], GS)\n",
        "            )\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_conv = nn.Conv2d(down_chs[0], img_ch, kernel_size=1)\n",
        "        print(f\"✅ Created UNet with {len(down_chs)} scale levels\")\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the UNet.\n",
        "        \"\"\"\n",
        "        t_embed = self.time_embed(t)\n",
        "        c_embed = self.class_embed(c)\n",
        "        c_embed = c_embed * c_mask # Apply mask\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        skips = []\n",
        "        for down_block in self.downs:\n",
        "            skips.append(x)\n",
        "            x = down_block(x)\n",
        "\n",
        "        x = self.mids(x)\n",
        "        b, c_dim, h_dim, w_dim = x.shape\n",
        "\n",
        "        t_proj = self.mid_t_proj(t_embed).view(b, c_dim, 1, 1)\n",
        "        c_proj = self.mid_c_proj(c_embed).view(b, c_dim, 1, 1)\n",
        "        x = x + t_proj + c_proj\n",
        "\n",
        "        # Fixed forward pass logic\n",
        "        for up_block in self.ups:\n",
        "            skip = skips.pop()\n",
        "            x = up_block(x, skip)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "print(\"✅ All model classes (UNet and helpers) are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etKu6Y7wNIOU",
        "outputId": "607324c9-38b2-438b-a0a7-3c4f2ebf478c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All model classes (UNet and helpers) are defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# from torch.optim import Adam\n",
        "# model, device, IMG_SIZE, IMG_CH, n_steps, N_CLASSES,\n",
        "# train_loader, val_loader\n",
        "# )\n",
        "from torch.optim import Adam # Added import for the optimizer\n",
        "\n",
        "# Create our model and move it to GPU if available\n",
        "# NOTE: Ensure you have run the cell defining the corrected UNet class\n",
        "# (which uses Conv2d for downsampling instead of RearrangePoolBlock)\n",
        "model = UNet(\n",
        "    T=n_steps,                 # Number of diffusion time steps\n",
        "    img_ch=IMG_CH,             # Number of channels in our images (1 for grayscale, 3 for RGB)\n",
        "    img_size=IMG_SIZE,         # Size of input images (28 for MNIST, 32 for CIFAR-10)\n",
        "    down_chs=(32, 64, 128),    # Channel dimensions for each downsampling level\n",
        "    t_embed_dim=8,             # Dimension for time step embeddings\n",
        "    c_embed_dim=N_CLASSES      # Number of classes for conditioning\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Input resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Input channels: {IMG_CH}\")\n",
        "print(f\"Time steps: {n_steps}\")\n",
        "print(f\"Condition classes: {N_CLASSES}\")\n",
        "print(f\"GPU acceleration: {'Yes' if device.type == 'cuda' else 'No'}\")\n",
        "\n",
        "# Validate model parameters and estimate memory requirements\n",
        "# Hint: Create functions to count parameters and estimate memory usage\n",
        "\n",
        "# Enter your code here:\n",
        "def count_parameters(model):\n",
        "    \"\"\"Counts the total number of trainable parameters in a model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total Trainable Parameters: {total_params:,} (~{total_params/1e6:.2f} M)\") # Completed the f-string\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    # Memory already allocated just for the model weights\n",
        "    allocated_mb = torch.cuda.memory_allocated(device) / (1024**2)\n",
        "    print(f\"Model VRAM (weights only): {allocated_mb:.2f} MB\")\n",
        "    print(\"Note: Total VRAM usage during training will be much higher due to gradients,\")\n",
        "    print(\"      optimizer states (Adam), and batch activations.\")\n",
        "\n",
        "\n",
        "# Your code to verify data ranges and integrity\n",
        "# Hint: Create functions to check data ranges in training and validation data\n",
        "\n",
        "# Enter your code here:\n",
        "def check_data_loader(loader, name):\n",
        "    \"\"\"Grabs one batch and prints its properties to check integrity.\"\"\"\n",
        "    print(f\"\\n--- Checking {name} ---\")\n",
        "    try:\n",
        "        # Get one batch and move it to the CPU for checking\n",
        "        images, labels = next(iter(loader))\n",
        "        images, labels = images.cpu(), labels.cpu()\n",
        "\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Image data type:   {images.dtype}\")\n",
        "        print(f\"  Image min/max/mean: {images.min():.2f} / {images.max():.2f} / {images.mean():.2f}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Label data type:   {labels.dtype}\")\n",
        "        print(f\"  Label min/max:     {labels.min()} / {labels.max()}\")\n",
        "        print(f\"  Image has NaNs:    {torch.isnan(images).any()}\")\n",
        "        print(f\"  Image has Infs:    {torch.isinf(images).any()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error checking {name}: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"DATA LOADER INTEGRITY CHECK\")\n",
        "print(f\"{'='*50}\")\n",
        "check_data_loader(train_loader, \"Training Loader\")\n",
        "check_data_loader(val_loader, \"Validation Loader\")\n",
        "print(\"\\nCheck: Image min/max should be approx. [-1.0, 1.0].\")\n",
        "print(\"Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\")\n",
        "\n",
        "\n",
        "# Set up the optimizer with parameters tuned for diffusion models\n",
        "# Note: Lower learning rates tend to work better for diffusion models\n",
        "initial_lr = 0.001  # Starting learning rate\n",
        "weight_decay = 1e-5  # L2 regularization to prevent overfitting\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=initial_lr,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
        "# This helps fine-tune the model toward the end of training\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',              # Reduce LR when monitored value stops decreasing\n",
        "    factor=0.5,              # Multiply LR by this factor\n",
        "    patience=5,              # Number of epochs with no improvement after which LR will be reduced\n",
        "    # verbose=True,          # Removed deprecated argument\n",
        "    min_lr=1e-6              # Lower bound on the learning rate\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\")\n",
        "\n",
        "# STUDENT EXPERIMENT:\n",
        "# Try different channel configurations and see how they affect:\n",
        "# 1. Model size (parameter count)\n",
        "# 2. Training time\n",
        "# 3. Generated image quality\n",
        "#\n",
        "# Suggestions:\n",
        "# - Smaller: down_chs=(16, 32, 64)\n",
        "# - Larger: down_chs=(64, 128, 256, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3J1Bj3f16dbh",
        "outputId": "b1fca100-e454-4e4b-8d8d-23a25b3fa0cf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created UNet with 3 scale levels\n",
            "\n",
            "==================================================\n",
            "MODEL ARCHITECTURE SUMMARY\n",
            "==================================================\n",
            "Input resolution: 28x28\n",
            "Input channels: 1\n",
            "Time steps: 100\n",
            "Condition classes: 10\n",
            "GPU acceleration: Yes\n",
            "Total Trainable Parameters: 3,841,773 (~3.84 M)\n",
            "Model VRAM (weights only): 27.00 MB\n",
            "Note: Total VRAM usage during training will be much higher due to gradients,\n",
            "      optimizer states (Adam), and batch activations.\n",
            "\n",
            "==================================================\n",
            "DATA LOADER INTEGRITY CHECK\n",
            "==================================================\n",
            "\n",
            "--- Checking Training Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.74\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "--- Checking Validation Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.73\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "Check: Image min/max should be approx. [-1.0, 1.0].\n",
            "Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\n",
            "\n",
            "✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes you have already run:\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# )\n",
        "\n",
        "#Now let's implement the upsampling block for our U-Net architecture:\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block for decoding path in U-Net architecture.\n",
        "\n",
        "    This block:\n",
        "    1. Takes features from the decoding path and corresponding skip connection\n",
        "    2. Concatenates them along the channel dimension\n",
        "    3. Upsamples spatial dimensions by 2x using transposed convolution\n",
        "    4. Processes features through multiple convolutional blocks\n",
        "\n",
        "    Args:\n",
        "        in_chs (int): Number of input channels from the previous layer\n",
        "        out_chs (int): Number of output channels\n",
        "        group_size (int): Number of groups for GroupNorm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Your code to create the upsampling operation\n",
        "        # Hint: Use nn.ConvTranspose2d with kernel_size=2 and stride=2\n",
        "        # Note that the input channels will be 2 * in_chs due to concatenation\n",
        "\n",
        "        # This layer upsamples the input 'x' from the layer below.\n",
        "        # It takes 'in_chs' and produces 'out_chs' to match the skip connection.\n",
        "        # (Note: The prompt's docstring/hints are slightly confusing.\n",
        "        # A standard U-Net upsamples 'x' from [B, in_chs, H, W] to [B, out_chs, 2H, 2W],\n",
        "        # then concatenates with 'skip' [B, out_chs, 2H, 2W]\n",
        "        # making the input to the convs [B, 2*out_chs, 2H, 2W].\n",
        "        # We will follow the prompt's hint literally,\n",
        "        # assuming the 'skip' tensor has 'in_chs' channels.)\n",
        "\n",
        "        # Based on the hint/docstring, we upsample in_chs -> in_chs\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "\n",
        "        # Enter your code here: (This part is handled by self.up above)\n",
        "\n",
        "\n",
        "        # Your code to create the convolutional blocks\n",
        "        # Hint: Use multiple GELUConvBlocks in sequence\n",
        "\n",
        "        # Input to convs will be 2 * in_chs (from upsampled 'x' + 'skip')\n",
        "        # Output should be 'out_chs'\n",
        "        # We use two conv blocks to process the concatenated features\n",
        "        self.conv = nn.Sequential(\n",
        "            # First block reduces channels from 2*in_chs to out_chs\n",
        "            GELUConvBlock(2 * in_chs, out_chs, group_size),\n",
        "            # Second block refines the features at the out_chs dimension\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "\n",
        "        # Enter your code here: (This part is handled by self.conv above)\n",
        "\n",
        "        # Log the configuration for debugging\n",
        "        print(f\"Created UpBlock: in_chs={in_chs}, out_chs={out_chs}, spatial_increase=2x\")\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        \"\"\"\n",
        "        Forward pass through the UpBlock.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor from previous layer [B, in_chs, H, W]\n",
        "            skip (torch.Tensor): Skip connection tensor from encoder [B, in_chs, 2H, 2W]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor with shape [B, out_chs, 2H, 2W]\n",
        "        \"\"\"\n",
        "        # Your code for the forward pass\n",
        "        # Hint: Concatenate x and skip, then upsample and process\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # 1. Upsample x to match the spatial dimensions of skip\n",
        "        x_up = self.up(x) # Shape: [B, in_chs, 2H, 2W]\n",
        "\n",
        "        # 2. Concatenate along the channel dimension (dim=1)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1) # Shape: [B, 2*in_chs, 2H, 2W]\n",
        "\n",
        "        # 3. Process with convolutional blocks\n",
        "        return self.conv(x_cat) # Shape: [B, out_chs, 2H, 2W]"
      ],
      "metadata": {
        "id": "askl0PTUH3ft"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "gA0NRYyACnQE"
      },
      "outputs": [],
      "source": [
        "# Here we implement the time embedding block for our U-Net architecture:\n",
        "# Helps the model understand time steps in diffusion process\n",
        "class SinusoidalPositionEmbedBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates sinusoidal embeddings for time steps in diffusion process.\n",
        "\n",
        "    This embedding scheme is adapted from the Transformer architecture and\n",
        "    provides a unique representation for each time step that preserves\n",
        "    relative distance information.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Embedding dimension\n",
        "    \"\"\"\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "\n",
        "    def forward(self, time):\n",
        "        \"\"\"\n",
        "        Computes sinusoidal embeddings for given time steps.\n",
        "\n",
        "        Args:\n",
        "            time (torch.Tensor): Time steps tensor of shape [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Time embeddings of shape [batch_size, dim]\n",
        "        \"\"\"\n",
        "        device = time.device\n",
        "        half_dim = self.dim // 2\n",
        "        embeddings = torch.log(torch.tensor(10000.0, device=device)) / (half_dim - 1)\n",
        "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
        "        embeddings = time[:, None] * embeddings[None, :]\n",
        "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
        "        return embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HMYbgkfPCnQF"
      },
      "outputs": [],
      "source": [
        "# Helps the model understand which number/image to draw (class conditioning)\n",
        "class EmbedBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Creates embeddings for class conditioning in diffusion models.\n",
        "\n",
        "    This module transforms a one-hot or index representation of a class\n",
        "    into a rich embedding that can be added to feature maps.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Input dimension (typically number of classes)\n",
        "        emb_dim (int): Output embedding dimension\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, emb_dim):\n",
        "        super(EmbedBlock, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Your code to create the embedding layers\n",
        "        # Hint: Use nn.Linear layers with a GELU activation, followed by\n",
        "        # nn.Unflatten to reshape for broadcasting with feature maps\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Computes class embeddings for the given class indices.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Class indices or one-hot encodings [batch_size, input_dim]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Class embeddings of shape [batch_size, emb_dim, 1, 1]\n",
        "                          (ready to be added to feature maps)\n",
        "        \"\"\"\n",
        "        x = x.view(-1, self.input_dim)\n",
        "        return self.model(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "K3th8ND3CnQF"
      },
      "outputs": [],
      "source": [
        "# Main U-Net model that puts everything together\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for diffusion models with time and class conditioning.\n",
        "\n",
        "    This architecture follows the standard U-Net design with:\n",
        "    1. Downsampling path that reduces spatial dimensions\n",
        "    2. Middle processing blocks\n",
        "    3. Upsampling path that reconstructs spatial dimensions\n",
        "    4. Skip connections between symmetric layers\n",
        "\n",
        "    The model is conditioned on:\n",
        "    - Time step (where we are in the diffusion process)\n",
        "    - Class labels (what we want to generate)\n",
        "\n",
        "    Args:\n",
        "        T (int): Number of diffusion time steps\n",
        "        img_ch (int): Number of image channels\n",
        "        img_size (int): Size of input images\n",
        "        down_chs (list): Channel dimensions for each level of U-Net\n",
        "        t_embed_dim (int): Dimension for time embeddings\n",
        "        c_embed_dim (int): Dimension for class embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Your code to create the time embedding\n",
        "        # Hint: Use SinusoidalPositionEmbedBlock, nn.Linear, and nn.GELU in sequence\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code to create the class embedding\n",
        "        # Hint: Use the EmbedBlock class you defined earlier\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code to create the initial convolution\n",
        "        # Hint: Use GELUConvBlock to process the input image\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code to create the downsampling path\n",
        "        # Hint: Use nn.ModuleList with DownBlock for each level\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code to create the middle blocks\n",
        "        # Hint: Use GELUConvBlock twice to process features at lowest resolution\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code to create the upsampling path\n",
        "        # Hint: Use nn.ModuleList with UpBlock for each level (in reverse order)\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code to create the final convolution\n",
        "        # Hint: Use nn.Conv2d to project back to the original image channels\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        print(f\"Created UNet with {len(down_chs)} scale levels\")\n",
        "        print(f\"Channel dimensions: {down_chs}\")\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the UNet.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input noisy image [B, img_ch, H, W]\n",
        "            t (torch.Tensor): Diffusion time steps [B]\n",
        "            c (torch.Tensor): Class labels [B, c_embed_dim]\n",
        "            c_mask (torch.Tensor): Mask for conditional generation [B, 1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted noise in the input image [B, img_ch, H, W]\n",
        "        \"\"\"\n",
        "        # Your code for the time embedding\n",
        "        # Hint: Process the time steps through the time embedding module\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code for the class embedding\n",
        "        # Hint: Process the class labels through the class embedding module\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code for the initial feature extraction\n",
        "        # Hint: Apply initial convolution to the input\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code for the downsampling path and skip connections\n",
        "        # Hint: Process the features through each downsampling block\n",
        "        # and store the outputs for skip connections\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code for the middle processing and conditioning\n",
        "        # Hint: Process features through middle blocks, then add time and class embeddings\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code for the upsampling path with skip connections\n",
        "        # Hint: Process features through each upsampling block,\n",
        "        # combining with corresponding skip connections\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # Your code for the final projection\n",
        "        # Hint: Apply the final convolution to get output in image space\n",
        "\n",
        "        # Enter your code here:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes 'torch', 'torch.nn as nn', 'N_CLASSES',\n",
        "# 'GELUConvBlock', 'DownBlock', and 'UpBlock' are defined)\n",
        "\n",
        "# Main U-Net model that puts everything together\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for diffusion models with time and class conditioning.\n",
        "    ... (docstring) ...\n",
        "    \"\"\"\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define a standard group size for GroupNorm\n",
        "        GS = 8\n",
        "        self.down_chs = down_chs\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Class embedding (assumes N_CLASSES is globally defined)\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.init_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Downsampling path\n",
        "        self.downs = nn.ModuleList()\n",
        "        for i in range(len(down_chs) - 1):\n",
        "            self.downs.append(DownBlock(down_chs[i], down_chs[i+1], GS))\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mids = nn.Sequential(\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS), # Fixed this line\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS)  # Fixed this line\n",
        "        )\n",
        "\n",
        "        # Linear layers to project embeddings\n",
        "        self.mid_t_proj = nn.Linear(t_embed_dim, down_chs[-1])\n",
        "        self.mid_c_proj = nn.Linear(c_embed_dim, down_chs[-1])\n",
        "\n",
        "        # Upsampling path\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(len(down_chs)-1, 0, -1):\n",
        "            self.ups.append(UpBlock(down_chs[i], down_chs[i-1], GS))\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_conv = nn.Conv2d(down_chs[0], img_ch, kernel_size=1)\n",
        "\n",
        "        print(f\"✅ Created UNet with {len(down_chs)} scale levels\")\n",
        "        print(f\"   Channel dimensions: {down_chs}\")\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the UNet.\n",
        "        \"\"\"\n",
        "\n",
        "        # Time embedding\n",
        "        t_embed = self.time_embed(t)\n",
        "\n",
        "        # Class embedding\n",
        "        c_embed = self.class_embed(c)\n",
        "        c_embed = c_embed * c_mask\n",
        "\n",
        "        # Initial feature extraction\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        # Downsampling path\n",
        "        skips = []\n",
        "        for down_block in self.downs:\n",
        "            skips.append(x)\n",
        "            x = down_block(x)\n",
        "\n",
        "        # Middle processing\n",
        "        x = self.mids(x)\n",
        "        b, c_dim, h_dim, w_dim = x.shape\n",
        "\n",
        "        # Project embeddings and reshape\n",
        "        t_proj = self.mid_t_proj(t_embed).view(b, c_dim, 1, 1)\n",
        "        c_proj = self.mid_c_proj(c_embed).view(b, c_dim, 1, 1)\n",
        "\n",
        "        # Add conditioning\n",
        "        x = x + t_proj + c_proj\n",
        "\n",
        "        # Upsampling path\n",
        "        for up_block in self.ups:\n",
        "            skip = skips.pop()\n",
        "            x = up_block(x, skip)\n",
        "\n",
        "        # Final projection\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "U8BG0PGX2lsJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes 'torch', 'torch.nn as nn', 'N_CLASSES',\n",
        "# 'GELUConvBlock', 'DownBlock', and 'UpBlock' are defined)\n",
        "\n",
        "# Main U-Net model that puts everything together\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for diffusion models with time and class conditioning.\n",
        "    ... (docstring) ...\n",
        "    \"\"\"\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define a standard group size for GroupNorm\n",
        "        GS = 8\n",
        "        self.down_chs = down_chs\n",
        "\n",
        "        # Your code to create the time embedding\n",
        "        # Hint: Use SinusoidalPositionEmbedBlock, nn.Linear, and nn.GELU in sequence\n",
        "        # We use nn.Embedding for discrete time steps + an MLP\n",
        "        # Enter your code here:\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Your code to create the class embedding\n",
        "        # Hint: Use the EmbedBlock class you defined earlier\n",
        "        # We'll use a standard nn.Embedding for class labels\n",
        "        # Enter your code here:\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "\n",
        "        # Your code to create the initial convolution\n",
        "        # Hint: Use GELUConvBlock to process the input image\n",
        "        # This maps the input image (e.g., 1 channel) to the first U-Net dim (e.g., 32)\n",
        "        # Enter your code here:\n",
        "        self.init_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Your code to create the downsampling path\n",
        "        # Hint: Use nn.ModuleList with DownBlock for each level\n",
        "        # Enter your code here:\n",
        "        self.downs = nn.ModuleList()\n",
        "        for i in range(len(down_chs) - 1):\n",
        "            # e.g., DownBlock(32, 64, 8), then DownBlock(64, 128, 8)\n",
        "            self.downs.append(DownBlock(down_chs[i], down_chs[i+1], GS))\n",
        "\n",
        "        # Your code to create the middle blocks\n",
        "        # Hint: Use GELUConvBlock twice to process features at lowest resolution\n",
        "        # Also create projection layers for time and class embeddings\n",
        "        # Enter your code here:\n",
        "        self.mids = nn.Sequential(\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS),\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS)\n",
        "        )\n",
        "\n",
        "        # Linear layers to project embeddings to match the middle channel dimension\n",
        "        self.mid_t_proj = nn.Linear(t_embed_dim, down_chs[-1])\n",
        "        self.mid_c_proj = nn.Linear(c_embed_dim, down_chs[-1])\n",
        "\n",
        "\n",
        "        # Your code to create the upsampling path\n",
        "        # Hint: Use nn.ModuleList with UpBlock for each level (in reverse order)\n",
        "        # Enter your code here:\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in reversed(range(len(down_chs) - 1)):\n",
        "            # e.g., UpBlock(128, 64, 8), then UpBlock(64, 32, 8)\n",
        "            self.ups.append(UpBlock(down_chs[i+1], down_chs[i], GS))\n",
        "\n",
        "        # Your code to create the final convolution\n",
        "        # Hint: Use nn.Conv2d to project back to the original image channels\n",
        "        # This maps the final U-Net dim (e.g., 32) back to image channels (e.g., 1)\n",
        "        # Enter your code here:\n",
        "        self.final_conv = nn.Conv2d(down_chs[0], img_ch, kernel_size=1)\n",
        "\n",
        "        print(f\"✅ Created UNet with {len(down_chs)} scale levels\")\n",
        "        print(f\"   Channel dimensions: {down_chs}\")\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the UNet.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input noisy image [B, img_ch, H, W]\n",
        "            t (torch.Tensor): Diffusion time steps [B]\n",
        "            c (torch.Tensor): Class labels [B] (as indices, not one-hot)\n",
        "            c_mask (torch.Tensor): Mask for conditional generation [B, 1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted noise in the input image [B, img_ch, H, W]\n",
        "        \"\"\"\n",
        "\n",
        "        # Your code for the time embedding\n",
        "        # Hint: Process the time steps through the time embedding module\n",
        "        # t_embed shape: [B, t_embed_dim]\n",
        "        # Enter your code here:\n",
        "        t_embed = self.time_embed(t)\n",
        "\n",
        "        # Your code for the class embedding\n",
        "        # Hint: Process the class labels through the class embedding module\n",
        "        # c_embed shape: [B, c_embed_dim]\n",
        "        # Enter your code here:\n",
        "        c_embed = self.class_embed(c)\n",
        "        c_embed = c_embed * c_mask # Apply classifier-free guidance mask\n",
        "\n",
        "        # Your code for the initial feature extraction\n",
        "        # Hint: Apply initial convolution to the input\n",
        "        # x shape: [B, down_chs[0], H, W]\n",
        "        # Enter your code here:\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        # Your code for the downsampling path and skip connections\n",
        "        # Hint: Process the features through each downsampling block\n",
        "        # and store the outputs for skip connections\n",
        "        # Enter your code here:\n",
        "        skips = [] # List to store skip connections\n",
        "        for down_block in self.downs:\n",
        "            skips.append(x) # Store the input to the block (the skip connection)\n",
        "            x = down_block(x) # Pass x through the block\n",
        "\n",
        "        # Your code for the middle processing and conditioning\n",
        "        # Hint: Process features through middle blocks, then add time and class embeddings\n",
        "        # Enter your code here:\n",
        "\n",
        "        # 1. Process features at the bottleneck\n",
        "        x = self.mids(x)\n",
        "\n",
        "        # 2. Get shape for reshaping embeddings\n",
        "        b, c_dim, h_dim, w_dim = x.shape\n",
        "\n",
        "        # 3. Project embeddings and reshape to [B, C, 1, 1]\n",
        "        t_proj = self.mid_t_proj(t_embed).view(b, c_dim, 1, 1)\n",
        "        c_proj = self.mid_c_proj(c_embed).view(b, c_dim, 1, 1)\n",
        "\n",
        "        # 4. Add conditioning to the features\n",
        "        x = x + t_proj + c_proj\n",
        "\n",
        "        # Your code for the upsampling path with skip connections\n",
        "        # Hint: Process features through each upsampling block,\n",
        "        # combining with corresponding skip connections\n",
        "        # Enter your code here:\n",
        "        for up_block in self.ups:\n",
        "            skip = skips.pop() # Get the last skip connection (LIFO)\n",
        "            x = up_block(x, skip) # Pass x and skip to the upsampling block\n",
        "\n",
        "        # Your code for the final projection\n",
        "        # Hint: Apply the final convolution to get output in image space\n",
        "        # Output shape: [B, img_ch, H, W]\n",
        "        # Enter your code here:\n",
        "        return self.final_conv(x)"
      ],
      "metadata": {
        "id": "n9PZOOya8pRv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes you have already run:\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# from torch.utils.data import DataLoader, random_split\n",
        "# import os\n",
        "# )\n",
        "\n",
        "# (These are the classes you defined in previous steps)\n",
        "# class GELUConvBlock(nn.Module): ...\n",
        "# class RearrangePoolBlock(nn.Module): ...\n",
        "# class DownBlock(nn.Module): ...\n",
        "# class UpBlock(nn.Module): ...\n",
        "\n",
        "# (These are the variables you defined in previous steps)\n",
        "# N_CLASSES = 10\n",
        "# T = 300 (or whatever you set it to)\n",
        "# t_embed_dim = 128 (example)\n",
        "# c_embed_dim = 128 (example)\n",
        "# down_chs = [64, 128, 256] (example)\n",
        "# img_ch = 1\n",
        "# img_size = 28\n",
        "\n",
        "\n",
        "# Main U-Net model that puts everything together\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for diffusion models with time and class conditioning.\n",
        "\n",
        "    This architecture follows the standard U-Net design with:\n",
        "    1. Downsampling path that reduces spatial dimensions\n",
        "    2. Middle processing blocks\n",
        "    3. Upsampling path that reconstructs spatial dimensions\n",
        "    4. Skip connections between symmetric layers\n",
        "\n",
        "    The model is conditioned on:\n",
        "    - Time step (where we are in the diffusion process)\n",
        "    - Class labels (what we want to generate)\n",
        "\n",
        "    Args:\n",
        "        T (int): Number of diffusion time steps\n",
        "        img_ch (int): Number of image channels\n",
        "        img_size (int): Size of input images\n",
        "        down_chs (list): Channel dimensions for each level of U-Net\n",
        "        t_embed_dim (int): Dimension for time embeddings\n",
        "        c_embed_dim (int): Dimension for class embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        # Using 8 as a default, robust group size\n",
        "        GS = 8\n",
        "        self.down_chs = down_chs\n",
        "        self.t_embed_dim = t_embed_dim\n",
        "        self.c_embed_dim = c_embed_dim\n",
        "\n",
        "        # Your code to create the time embedding\n",
        "        # Hint: Use SinusoidalPositionEmbedBlock, nn.Linear, and nn.GELU in sequence\n",
        "        # We'll use nn.Embedding as the \"SinusoidalPositionEmbedBlock\" for discrete time\n",
        "        # This is a standard MLP for processing time\n",
        "        # Enter your code here:\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Your code to create the class embedding\n",
        "        # Hint: Use the EmbedBlock class you defined earlier\n",
        "        # We'll use nn.Embedding (assumes N_CLASSES is globally defined)\n",
        "        # Enter your code here:\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "\n",
        "        # Your code to create the initial convolution\n",
        "        # Hint: Use GELUConvBlock to process the input image\n",
        "        # Enter your code here:\n",
        "        self.initial_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Your code to create the downsampling path\n",
        "        # Hint: Use nn.ModuleList with DownBlock for each level\n",
        "        # Enter your code here:\n",
        "        self.downs = nn.ModuleList([\n",
        "            DownBlock(down_chs[i], down_chs[i+1], GS)\n",
        "            for i in range(len(down_chs)-1)\n",
        "        ])\n",
        "\n",
        "        # Your code to create the middle blocks\n",
        "        # Hint: Use GELUConvBlock twice to process features at lowest resolution\n",
        "        # Enter your code here:\n",
        "        self.middle = nn.Sequential(\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS),\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS)\n",
        "        )\n",
        "\n",
        "        # Your code to create the upsampling path\n",
        "        # Hint: Use nn.ModuleList with UpBlock for each level (in reverse order)\n",
        "        # Enter your code here:\n",
        "        self.ups = nn.ModuleList([\n",
        "            UpBlock(down_chs[i+1], down_chs[i], GS)\n",
        "            for i in range(len(down_chs)-1, 0, -1)\n",
        "        ])\n",
        "\n",
        "        # Your code to create the final convolution\n",
        "        # Hint: Use nn.Conv2d to project back to the original image channels\n",
        "        # Enter your code here:\n",
        "        # Final convolution to map channels back to image channels\n",
        "        # It takes the output from the last UpBlock (down_chs[0] channels)\n",
        "        # and outputs img_ch channels (e.g., 1 for MNIST)\n",
        "        self.final_conv = nn.Conv2d(down_chs[0], img_ch, kernel_size=1)\n",
        "\n",
        "\n",
        "        print(f\"Created UNet with {len(down_chs)} scale levels\")\n",
        "        print(f\"Channel dimensions: {down_chs}\")\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the UNet.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input noisy image [B, img_ch, H, W]\n",
        "            t (torch.Tensor): Diffusion time steps [B]\n",
        "            c (torch.Tensor): Class labels [B, c_embed_dim]\n",
        "            c_mask (torch.Tensor): Mask for conditional generation [B, 1]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Predicted noise in the input image [B, img_ch, H, W]\n",
        "        \"\"\"\n",
        "        # Your code for the time embedding\n",
        "        # Hint: Process the time steps through the time embedding module\n",
        "\n",
        "        # Enter your code here:\n",
        "        t_emb = self.time_embed(t) # Shape: [B, t_embed_dim]\n",
        "\n",
        "\n",
        "        # Your code for the class embedding\n",
        "        # Hint: Process the class labels through the class embedding module\n",
        "\n",
        "        # Enter your code here:\n",
        "        c_emb = self.class_embed(c) # Shape: [B, c_embed_dim]\n",
        "\n",
        "        # Apply class conditioning mask\n",
        "        # This sets class embedding to zero for masked samples (used in Classifier-Free Guidance)\n",
        "        # We need to reshape c_mask to match the embedding shape for broadcasting\n",
        "        c_mask = c_mask.view(-1, 1) # Ensure c_mask is [B, 1]\n",
        "        c_emb = c_emb * c_mask # Shape: [B, c_embed_dim]\n",
        "\n",
        "\n",
        "        # Your code for the initial feature extraction\n",
        "        # Hint: Apply initial convolution to the input\n",
        "\n",
        "        # Enter your code here:\n",
        "        x = self.initial_conv(x) # Shape: [B, down_chs[0], H, W]\n",
        "\n",
        "\n",
        "        # Your code for the downsampling path and skip connections\n",
        "        # Hint: Process the features through each downsampling block\n",
        "        # and store the outputs for skip connections\n",
        "\n",
        "        # Enter your code here:\n",
        "        skips = [x] # Store initial conv output as the first skip connection\n",
        "        for down in self.downs:\n",
        "            x = down(x)\n",
        "            skips.append(x) # Store output of each down block as a skip connection\n",
        "\n",
        "\n",
        "        # Your code for the middle processing and conditioning\n",
        "        # Hint: Process features through middle blocks, then add time and class embeddings\n",
        "\n",
        "        # Enter your code here:\n",
        "        x = self.middle(x) # Process through middle blocks\n",
        "        # Add time and class embeddings to the middle features\n",
        "        # We need to reshape embeddings to match feature map dimensions for broadcasting\n",
        "        x = x + t_emb.view(-1, self.t_embed_dim, 1, 1)\n",
        "        x = x + c_emb.view(-1, self.c_embed_dim, 1, 1)\n",
        "\n",
        "\n",
        "        # Your code for the upsampling path with skip connections\n",
        "        # Hint: Process features through each upsampling block,\n",
        "        # combining with corresponding skip connections\n",
        "\n",
        "        # Enter your code here:\n",
        "        # Process through upsampling blocks, combining with skip connections from the encoder\n",
        "        # Note: We iterate through up blocks and corresponding skip connections in reverse order\n",
        "        for i, up in enumerate(self.ups):\n",
        "            # The skip connections are stored from shallowest to deepest (skips[0] to skips[-1])\n",
        "            # We need to use them from deepest to shallowest for upsampling\n",
        "            skip_connection = skips[-(i+1)] # Get the correct skip connection\n",
        "            x = up(x, skip_connection) # Pass current features and skip connection to UpBlock\n",
        "\n",
        "\n",
        "        # Your code for the final projection\n",
        "        # Hint: Apply the final convolution to get output in image space\n",
        "\n",
        "        # Enter your code here:\n",
        "        output = self.final_conv(x) # Project back to image channels\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "dfu3v0bBIkxY"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV3_WkxA4JM9"
      },
      "source": [
        "## Step 4: Setting Up The Diffusion Process\n",
        "\n",
        "Now we'll create the process of adding and removing noise from images. Think of it like:\n",
        "1. Adding fog: Slowly making the image more and more blurry until you can't see it\n",
        "2. Removing fog: Teaching the AI to gradually make the image clearer\n",
        "3. Controlling the process: Making sure we can generate specific numbers we want"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "zN1EXodBCnQG"
      },
      "outputs": [],
      "source": [
        "# Set up the noise schedule\n",
        "n_steps = 100  # How many steps to go from clear image to noise\n",
        "beta_start = 0.0001  # Starting noise level (small)\n",
        "beta_end = 0.02      # Ending noise level (larger)\n",
        "\n",
        "# Create schedule of gradually increasing noise levels\n",
        "beta = torch.linspace(beta_start, beta_end, n_steps).to(device)\n",
        "\n",
        "# Calculate important values used in diffusion equations\n",
        "alpha = 1 - beta  # Portion of original image to keep at each step\n",
        "alpha_bar = torch.cumprod(alpha, dim=0)  # Cumulative product of alphas\n",
        "sqrt_alpha_bar = torch.sqrt(alpha_bar)  # For scaling the original image\n",
        "sqrt_one_minus_alpha_bar = torch.sqrt(1 - alpha_bar)  # For scaling the noise\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes 'torch' is imported and 'sqrt_alpha_bar' and\n",
        "#  'sqrt_one_minus_alpha_bar' are globally defined Tensors)\n",
        "\n",
        "# Function to add noise to images (forward diffusion process)\n",
        "def add_noise(x_0, t):\n",
        "    \"\"\"\n",
        "    Add noise to images according to the forward diffusion process.\n",
        "\n",
        "    The formula is: x_t = √(α_bar_t) * x_0 + √(1-α_bar_t) * ε\n",
        "    where ε is random noise and α_bar_t is the cumulative product of (1-β).\n",
        "\n",
        "    Args:\n",
        "        x_0 (torch.Tensor): Original clean image [B, C, H, W]\n",
        "        t (torch.Tensor): Timestep indices indicating noise level [B]\n",
        "\n",
        "    Returns:\n",
        "        tuple: (noisy_image, noise_added)\n",
        "            - noisy_image is the image with noise added\n",
        "            - noise_added is the actual noise that was added (for training)\n",
        "    \"\"\"\n",
        "    # Create random Gaussian noise with same shape as image\n",
        "    noise = torch.randn_like(x_0)\n",
        "\n",
        "    # Get noise schedule values for the specified timesteps\n",
        "    # Reshape to allow broadcasting with image dimensions\n",
        "    sqrt_alpha_bar_t = sqrt_alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "    sqrt_one_minus_alpha_bar_t = sqrt_one_minus_alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "\n",
        "    # Apply the forward diffusion equation:\n",
        "    # Mixture of original image (scaled down) and noise (scaled up)     # Your code to apply the forward diffusion equation\n",
        "    # Hint: Mix the original image and noise according to the noise schedule\n",
        "\n",
        "    # Enter your code here:\n",
        "    # This line is the exact formula from the docstring:\n",
        "    # x_t = (signal part) + (noise part)\n",
        "    x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise\n",
        "\n",
        "    return x_t, noise"
      ],
      "metadata": {
        "id": "n7XM99w6PLLi"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes 'sqrt_alpha_bar' and 'sqrt_one_minus_alpha_bar' are\n",
        "#  torch.Tensors defined globally in your script)\n",
        "# (This also assumes 'torch' is imported)\n",
        "\n",
        "# Function to add noise to images (forward diffusion process)\n",
        "def add_noise(x_0, t):\n",
        "    \"\"\"\n",
        "    Add noise to images according to the forward diffusion process.\n",
        "\n",
        "    The formula is: x_t = √(α_bar_t) * x_0 + √(1-α_bar_t) * ε\n",
        "    where ε is random noise and α_bar_t is the cumulative product of (1-β).\n",
        "\n",
        "    Args:\n",
        "        x_0 (torch.Tensor): Original clean image [B, C, H, W]\n",
        "        t (torch.Tensor): Timestep indices indicating noise level [B]\n",
        "\n",
        "    Returns:\n",
        "        tuple: (noisy_image, noise_added)\n",
        "            - noisy_image is the image with noise added\n",
        "            - noise_added is the actual noise that was added (for training)\n",
        "    \"\"\"\n",
        "    # Create random Gaussian noise with same shape as image\n",
        "    noise = torch.randn_like(x_0)\n",
        "\n",
        "    # Get noise schedule values for the specified timesteps\n",
        "    # Reshape to allow broadcasting with image dimensions\n",
        "    sqrt_alpha_bar_t = sqrt_alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "    sqrt_one_minus_alpha_bar_t = sqrt_one_minus_alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "\n",
        "    # Apply the forward diffusion equation:\n",
        "    # Mixture of original image (scaled down) and noise (scaled up)\n",
        "    # Your code to apply the forward diffusion equation\n",
        "    # Hint: Mix the original image and noise according to the noise schedule\n",
        "\n",
        "    # Enter your code here:\n",
        "    x_t = sqrt_alpha_bar_t * x_0 + sqrt_one_minus_alpha_bar_t * noise\n",
        "\n",
        "    return x_t, noise"
      ],
      "metadata": {
        "id": "alm1YLleI2yM"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "ufjqiGasCnQG"
      },
      "outputs": [],
      "source": [
        "# Function to remove noise from images (reverse diffusion process)\n",
        "@torch.no_grad()  # Don't track gradients during sampling (inference only)\n",
        "def remove_noise(x_t, t, model, c, c_mask):\n",
        "    \"\"\"\n",
        "    Remove noise from images using the learned reverse diffusion process.\n",
        "\n",
        "    This implements a single step of the reverse diffusion sampling process.\n",
        "    The model predicts the noise in the image, which we then use to partially\n",
        "    denoise the image.\n",
        "\n",
        "    Args:\n",
        "        x_t (torch.Tensor): Noisy image at timestep t [B, C, H, W]\n",
        "        t (torch.Tensor): Current timestep indices [B]\n",
        "        model (nn.Module): U-Net model that predicts noise\n",
        "        c (torch.Tensor): Class conditioning (what digit to generate) [B] - should be Long tensor\n",
        "        c_mask (torch.Tensor): Mask for conditional generation [B, 1] - should be Float tensor\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Less noisy image for the next timestep [B, C, H, W]\n",
        "    \"\"\"\n",
        "    # Predict the noise in the image using our model\n",
        "    # Pass 'c' (Long indices) and 'c_mask' (Float)\n",
        "    predicted_noise = model(x_t, t, c, c_mask) # Fixed: Pass 'c' instead of 'c_one_hot' from generate_samples\n",
        "\n",
        "    # Get noise schedule values for the current timestep\n",
        "    alpha_t = alpha[t].reshape(-1, 1, 1, 1)\n",
        "    alpha_bar_t = alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "    beta_t = beta[t].reshape(-1, 1, 1, 1)\n",
        "\n",
        "    # Special case: if we're at the first timestep (t=0), we're done\n",
        "    if t[0] == 0:\n",
        "        return x_t\n",
        "    else:\n",
        "        # Calculate the mean of the denoised distribution\n",
        "        # This is derived from Bayes' rule and the diffusion process equations\n",
        "        mean = (1 / torch.sqrt(alpha_t)) * (\n",
        "            x_t - (beta_t / sqrt_one_minus_alpha_bar_t) * predicted_noise\n",
        "        )\n",
        "\n",
        "        # Add a small amount of random noise (variance depends on timestep)\n",
        "        # This helps prevent the generation from becoming too deterministic\n",
        "        noise = torch.randn_like(x_t)\n",
        "\n",
        "        # Return the partially denoised image with a bit of new random noise\n",
        "        return mean + torch.sqrt(beta_t) * noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "yxWjA0q2CnQH"
      },
      "outputs": [],
      "source": [
        "# Function to remove noise from images (reverse diffusion process)\n",
        "@torch.no_grad()  # Don't track gradients during sampling (inference only)\n",
        "def remove_noise(x_t, t, model, c, c_mask):\n",
        "    \"\"\"\n",
        "    Remove noise from images using the learned reverse diffusion process.\n",
        "\n",
        "    This implements a single step of the reverse diffusion sampling process.\n",
        "    The model predicts the noise in the image, which we then use to partially\n",
        "    denoise the image.\n",
        "\n",
        "    Args:\n",
        "        x_t (torch.Tensor): Noisy image at timestep t [B, C, H, W]\n",
        "        t (torch.Tensor): Current timestep indices [B]\n",
        "        model (nn.Module): U-Net model that predicts noise\n",
        "        c (torch.Tensor): Class conditioning (what digit to generate) [B, C]\n",
        "        c_mask (torch.Tensor): Mask for conditional generation [B, 1]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Less noisy image for the next timestep [B, C, H, W]\n",
        "    \"\"\"\n",
        "    # Predict the noise in the image using our model\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "\n",
        "    # Get noise schedule values for the current timestep\n",
        "    alpha_t = alpha[t].reshape(-1, 1, 1, 1)\n",
        "    alpha_bar_t = alpha_bar[t].reshape(-1, 1, 1, 1)\n",
        "    beta_t = beta[t].reshape(-1, 1, 1, 1)\n",
        "\n",
        "    # Special case: if we're at the first timestep (t=0), we're done\n",
        "    if t[0] == 0:\n",
        "        return x_t\n",
        "    else:\n",
        "        # Calculate the mean of the denoised distribution\n",
        "        # This is derived from Bayes' rule and the diffusion process equations\n",
        "        mean = (1 / torch.sqrt(alpha_t)) * (\n",
        "            x_t - (beta_t / sqrt_one_minus_alpha_bar_t) * predicted_noise\n",
        "        )\n",
        "\n",
        "        # Add a small amount of random noise (variance depends on timestep)\n",
        "        # This helps prevent the generation from becoming too deterministic\n",
        "        noise = torch.randn_like(x_t)\n",
        "\n",
        "        # Return the partially denoised image with a bit of new random noise\n",
        "        return mean + torch.sqrt(beta_t) * noise"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# from\n",
        "# n_steps = ... (e.g., 300)\n",
        "# device = ... (e.g., 'cuda')\n",
        "# IMG_CH = ... (e.g., 1)\n",
        "# add_noise = ... (the function you defined)\n",
        "# train_loader = ... (the DataLoader you defined) # Corrected variable name\n",
        "# )\n",
        "\n",
        "# Visualization function to show how noise progressively affects images\n",
        "def show_noise_progression(image, num_steps=5):\n",
        "    \"\"\"\n",
        "    Visualize how an image gets progressively noisier in the diffusion process.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Original clean image [C, H, W]\n",
        "        num_steps (int): Number of noise levels to show\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 3))\n",
        "\n",
        "    # Show original image\n",
        "    plt.subplot(1, num_steps, 1)\n",
        "    if IMG_CH == 1:  # Grayscale image\n",
        "        plt.imshow(image[0].cpu(), cmap='gray')\n",
        "    else:  # Color image\n",
        "        img = image.permute(1, 2, 0).cpu()  # Change from [C,H,W] to [H,W,C]\n",
        "        if img.min() < 0:  # If normalized between -1 and 1\n",
        "            img = (img + 1) / 2  # Rescale to [0,1] for display\n",
        "        plt.imshow(img)\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Show progressively noisier versions\n",
        "    for i in range(1, num_steps):\n",
        "        # Calculate timestep index based on percentage through the process\n",
        "        # Ensure t_idx is within the valid range [0, n_steps-1]\n",
        "        t_idx = int((i / (num_steps - 1)) * (n_steps - 1))\n",
        "        t = torch.tensor([t_idx]).to(device)\n",
        "\n",
        "        # Add noise corresponding to timestep t\n",
        "        noisy_image, _ = add_noise(image.unsqueeze(0).to(device), t) # Ensure image is on device\n",
        "\n",
        "        # Display the noisy image\n",
        "        plt.subplot(1, num_steps, i + 1)\n",
        "        if IMG_CH == 1:\n",
        "            plt.imshow(noisy_image[0][0].cpu(), cmap='gray')\n",
        "        else:\n",
        "            img = noisy_image[0].permute(1, 2, 0).cpu()\n",
        "            if img.min() < 0:\n",
        "                img = (img + 1) / 2\n",
        "            plt.imshow(img)\n",
        "        plt.title(f'{int((i/(num_steps-1)) * n_steps)}% Noise') # Fixed title calculation\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Show an example of noise progression on a real image\n",
        "sample_batch = next(iter(train_loader))  # Get first batch (Corrected variable name)\n",
        "sample_image = sample_batch[0][0].to(device)  # Get first image\n",
        "show_noise_progression(sample_image)\n",
        "\n",
        "# Student Activity: Try different noise schedules\n",
        "# Uncomment and modify these lines to experiment:\n",
        "\"\"\"\n",
        "# Try a non-linear noise schedule\n",
        "beta_alt = torch.linspace(beta_start, beta_end, n_steps)**2\n",
        "alpha_alt = 1 - beta_alt\n",
        "alpha_bar_alt = torch.cumprod(alpha_alt, dim=0)\n",
        "# How would this affect the diffusion process?\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "AwM0GGOXPauR",
        "outputId": "f3df06e6-407e-45ba-c746-4813070b0db8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x300 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Try a non-linear noise schedule\\nbeta_alt = torch.linspace(beta_start, beta_end, n_steps)**2\\nalpha_alt = 1 - beta_alt\\nalpha_bar_alt = torch.cumprod(alpha_alt, dim=0)\\n# How would this affect the diffusion process?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization function to show how noise progressively affects images\n",
        "def show_noise_progression(image, num_steps=5):\n",
        "    \"\"\"\n",
        "    Visualize how an image gets progressively noisier in the diffusion process.\n",
        "\n",
        "    Args:\n",
        "        image (torch.Tensor): Original clean image [C, H, W]\n",
        "        num_steps (int): Number of noise levels to show\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(15, 3))\n",
        "\n",
        "    # Show original image\n",
        "    plt.subplot(1, num_steps, 1)\n",
        "    if IMG_CH == 1:  # Grayscale image\n",
        "        plt.imshow(image[0].cpu(), cmap='gray')\n",
        "    else:  # Color image\n",
        "        img = image.permute(1, 2, 0).cpu()  # Change from [C,H,W] to [H,W,C]\n",
        "        if img.min() < 0:  # If normalized between -1 and 1\n",
        "            img = (img + 1) / 2  # Rescale to [0,1] for display\n",
        "        plt.imshow(img)\n",
        "    plt.title('Original')\n",
        "    plt.axis('off')\n",
        "\n",
        "    # Show progressively noisier versions\n",
        "    for i in range(1, num_steps):\n",
        "        # Calculate timestep index based on percentage through the process\n",
        "        t_idx = int((i/(num_steps-1)) * (n_steps-1)) # Corrected logic to span 0 to n_steps-1\n",
        "        t = torch.tensor([t_idx]).to(device)\n",
        "\n",
        "        # Add noise corresponding to timestep t\n",
        "        noisy_image, _ = add_noise(image.unsqueeze(0).to(device), t)\n",
        "\n",
        "        # Display the noisy image\n",
        "        plt.subplot(1, num_steps, i+1)\n",
        "        if IMG_CH == 1:\n",
        "            plt.imshow(noisy_image[0][0].cpu(), cmap='gray')\n",
        "        else:\n",
        "            # THIS IS THE FIXED LINE:\n",
        "            img = noisy_image[0].permute(1, 2, 0).cpu() # Added closing ')'\n",
        "\n",
        "            if img.min() < 0:\n",
        "                img = (img + 1) / 2\n",
        "            plt.imshow(img)\n",
        "        plt.title(f't={t_idx} (~{int((i/(num_steps-1)) * 100)}% Noise)')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kaR8x7qxJan_"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ra5ulIc44V6A"
      },
      "source": [
        "## Step 5: Training Our Model\n",
        "\n",
        "Now we'll teach our AI to generate images. This process:\n",
        "1. Takes a clear image\n",
        "2. Adds random noise to it\n",
        "3. Asks our AI to predict what noise was added\n",
        "4. Helps our AI learn from its mistakes\n",
        "\n",
        "This will take a while, but we'll see progress as it learns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "_UTzbHp9CnQH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "7b606e43-7246-49d4-ddfd-aedb6df75ab2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "tuple index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2117198664.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create our model and move it to GPU if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = UNet(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m                 \u001b[0;31m# Number of diffusion time steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mimg_ch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_CH\u001b[0m\u001b[0;34m,\u001b[0m             \u001b[0;31m# Number of channels in our images (1 for grayscale, 3 for RGB)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_SIZE\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0;31m# Size of input images (28 for MNIST, 32 for CIFAR-10)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1796123897.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# Enter your code here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         self.ups = nn.ModuleList([\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mUpBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_chs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdown_chs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_chs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         ])\n",
            "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
          ]
        }
      ],
      "source": [
        "# Create our model and move it to GPU if available\n",
        "model = UNet(\n",
        "    T=n_steps,                 # Number of diffusion time steps\n",
        "    img_ch=IMG_CH,             # Number of channels in our images (1 for grayscale, 3 for RGB)\n",
        "    img_size=IMG_SIZE,         # Size of input images (28 for MNIST, 32 for CIFAR-10)\n",
        "    down_chs=(32, 64, 128),    # Channel dimensions for each downsampling level\n",
        "    t_embed_dim=8,             # Dimension for time step embeddings\n",
        "    c_embed_dim=N_CLASSES      # Number of classes for conditioning\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Input resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Input channels: {IMG_CH}\")\n",
        "print(f\"Time steps: {n_steps}\")\n",
        "print(f\"Condition classes: {N_CLASSES}\")\n",
        "print(f\"GPU acceleration: {'Yes' if device.type == 'cuda' else 'No'}\")\n",
        "\n",
        "# Validate model parameters and estimate memory requirements\n",
        "# Hint: Create functions to count parameters and estimate memory usage\n",
        "\n",
        "# Enter your code here:\n",
        "\n",
        "# Your code to verify data ranges and integrity\n",
        "# Hint: Create functions to check data ranges in training and validation data\n",
        "\n",
        "# Enter your code here:\n",
        "\n",
        "\n",
        "# Set up the optimizer with parameters tuned for diffusion models\n",
        "# Note: Lower learning rates tend to work better for diffusion models\n",
        "initial_lr = 0.001  # Starting learning rate\n",
        "weight_decay = 1e-5  # L2 regularization to prevent overfitting\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=initial_lr,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
        "# This helps fine-tune the model toward the end of training\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',              # Reduce LR when monitored value stops decreasing\n",
        "    factor=0.5,              # Multiply LR by this factor\n",
        "    patience=5,              # Number of epochs with no improvement after which LR will be reduced\n",
        "    verbose=True,            # Print message when LR is reduced\n",
        "    min_lr=1e-6              # Lower bound on the learning rate\n",
        ")\n",
        "\n",
        "# STUDENT EXPERIMENT:\n",
        "# Try different channel configurations and see how they affect:\n",
        "# 1. Model size (parameter count)\n",
        "# 2. Training time\n",
        "# 3. Generated image quality\n",
        "#\n",
        "# Suggestions:\n",
        "# - Smaller: down_chs=(16, 32, 64)\n",
        "# - Larger: down_chs=(64, 128, 256, 512)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# from torch.optim import Adam\n",
        "# model, device, IMG_SIZE, IMG_CH, n_steps, N_CLASSES,\n",
        "# train_loader, val_loader, UNet\n",
        "# )\n",
        "from torch.optim import Adam # Added import for the optimizer\n",
        "\n",
        "# Create our model and move it to GPU if available\n",
        "model = UNet(\n",
        "    T=n_steps,                 # Number of diffusion time steps\n",
        "    img_ch=IMG_CH,             # Number of channels in our images (1 for grayscale, 3 for RGB)\n",
        "    img_size=IMG_SIZE,         # Size of input images (28 for MNIST, 32 for CIFAR-10)\n",
        "    down_chs=(32, 64, 128),    # Channel dimensions for each downsampling level\n",
        "    t_embed_dim=8,             # Dimension for time step embeddings\n",
        "    c_embed_dim=N_CLASSES      # Number of classes for conditioning\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Input resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Input channels: {IMG_CH}\")\n",
        "print(f\"Time steps: {n_steps}\")\n",
        "print(f\"Condition classes: {N_CLASSES}\")\n",
        "print(f\"GPU acceleration: {'Yes' if device.type == 'cuda' else 'No'}\")\n",
        "\n",
        "# Validate model parameters and estimate memory requirements\n",
        "# Hint: Create functions to count parameters and estimate memory usage\n",
        "\n",
        "# Enter your code here:\n",
        "def count_parameters(model):\n",
        "    \"\"\"Counts the total number of trainable parameters in a model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total Trainable Parameters: {total_params:,} (~{total_params/1e6:.2f} M)\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    # Memory already allocated just for the model weights\n",
        "    allocated_mb = torch.cuda.memory_allocated(device) / (1024**2)\n",
        "    print(f\"Model VRAM (weights only): {allocated_mb:.2f} MB\")\n",
        "    print(\"Note: Total VRAM usage during training will be much higher due to gradients,\")\n",
        "    print(\"      optimizer states (Adam), and batch activations.\")\n",
        "\n",
        "\n",
        "# Your code to verify data ranges and integrity\n",
        "# Hint: Create functions to check data ranges in training and validation data\n",
        "\n",
        "# Enter your code here:\n",
        "def check_data_loader(loader, name):\n",
        "    \"\"\"Grabs one batch and prints its properties to check integrity.\"\"\"\n",
        "    print(f\"\\n--- Checking {name} ---\")\n",
        "    try:\n",
        "        # Get one batch and move it to the CPU for checking\n",
        "        images, labels = next(iter(loader))\n",
        "        images, labels = images.cpu(), labels.cpu()\n",
        "\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Image data type:   {images.dtype}\")\n",
        "        print(f\"  Image min/max/mean: {images.min():.2f} / {images.max():.2f} / {images.mean():.2f}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Label data type:   {labels.dtype}\")\n",
        "        print(f\"  Label min/max:     {labels.min()} / {labels.max()}\")\n",
        "        print(f\"  Image has NaNs:    {torch.isnan(images).any()}\")\n",
        "        print(f\"  Image has Infs:    {torch.isinf(images).any()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error checking {name}: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"DATA LOADER INTEGRITY CHECK\")\n",
        "print(f\"{'='*50}\")\n",
        "check_data_loader(train_loader, \"Training Loader\")\n",
        "check_data_loader(val_loader, \"Validation Loader\")\n",
        "print(\"\\nCheck: Image min/max should be approx. [-1.0, 1.0].\")\n",
        "print(\"Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\")\n",
        "\n",
        "\n",
        "# Set up the optimizer with parameters tuned for diffusion models\n",
        "# Note: Lower learning rates tend to work better for diffusion models\n",
        "initial_lr = 0.001  # Starting learning rate\n",
        "weight_decay = 1e-5  # L2 regularization to prevent overfitting\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=initial_lr,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
        "# This helps fine-tune the model toward the end of training\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',              # Reduce LR when monitored value stops decreasing\n",
        "    factor=0.5,              # Multiply LR by this factor\n",
        "    patience=5,              # Number of epochs with no improvement after which LR will be reduced\n",
        "    # verbose=True,          # <-- THIS LINE WAS REMOVED TO FIX THE TypeError\n",
        "    min_lr=1e-6              # Lower bound on the learning rate\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\")\n",
        "\n",
        "# STUDENT EXPERIMENT:\n",
        "# Try different channel configurations and see how they affect:\n",
        "# 1. Model size (parameter count)\n",
        "# 2. Training time\n",
        "# 3. Generated image quality\n",
        "#\n",
        "# Suggestions:\n",
        "# - Smaller: down_chs=(16, 32, 64)\n",
        "# - Larger: down_chs=(64, 128, 256, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkZfTtJG3orB",
        "outputId": "4526d182-1963-41e7-a0f6-1721baf899fe"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL ARCHITECTURE SUMMARY\n",
            "==================================================\n",
            "Input resolution: 28x28\n",
            "Input channels: 1\n",
            "Time steps: 100\n",
            "Condition classes: 10\n",
            "GPU acceleration: Yes\n",
            "Total Trainable Parameters: 3,528,652 (~3.53 M)\n",
            "Model VRAM (weights only): 28.14 MB\n",
            "Note: Total VRAM usage during training will be much higher due to gradients,\n",
            "      optimizer states (Adam), and batch activations.\n",
            "\n",
            "==================================================\n",
            "DATA LOADER INTEGRITY CHECK\n",
            "==================================================\n",
            "\n",
            "--- Checking Training Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.71\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "--- Checking Validation Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.73\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "Check: Image min/max should be approx. [-1.0, 1.0].\n",
            "Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\n",
            "\n",
            "✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        # Fix group_size if not divisible\n",
        "        if out_ch % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while out_ch % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if out_ch % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = 1\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        # Fix for EinopsError: Use named parameters p1=2, p2=2\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        # Fix group_size for new channel count\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "# 3. HELPER CLASS: DownBlock (THIS IS THE FIXED ONE)\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),  # <-- Fixed this line\n",
        "            RearrangePoolBlock(out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 4. HELPER CLASS: UpBlock\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            GELUConvBlock(2 * in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x_up = self.up(x)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.conv(x_cat)\n",
        "\n",
        "# 5. MAIN UNET CLASS\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "        GS = 8 # Default Group Size\n",
        "        self.down_chs = down_chs\n",
        "        self.t_embed_dim = t_embed_dim\n",
        "        self.c_embed_dim = c_embed_dim\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Class embedding (assumes N_CLASSES is globally defined)\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.init_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Downsampling path\n",
        "        self.downs = nn.ModuleList()\n",
        "        for i in range(len(down_chs) - 1):\n",
        "            self.downs.append(\n",
        "                DownBlock(down_chs[i], down_chs[i+1], GS)\n",
        "            )\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mids = nn.Sequential(\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS),\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS)\n",
        "        )\n",
        "        self.mid_t_proj = nn.Linear(t_embed_dim, down_chs[-1])\n",
        "        self.mid_c_proj = nn.Linear(c_embed_dim, down_chs[-1])\n",
        "\n",
        "        # Upsampling path (Fixed IndexError)\n",
        "        self.ups = nn.Module"
      ],
      "metadata": {
        "id": "rtneErig373J"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# from torch.optim import Adam\n",
        "# model, device, IMG_SIZE, IMG_CH, n_steps, N_CLASSES,\n",
        "# train_loader, val_loader, UNet\n",
        "# )\n",
        "from torch.optim import Adam # Added import for the optimizer\n",
        "\n",
        "# Create our model and move it to GPU if available\n",
        "model = UNet(\n",
        "    T=n_steps,                 # Number of diffusion time steps\n",
        "    img_ch=IMG_CH,             # Number of channels in our images (1 for grayscale, 3 for RGB)\n",
        "    img_size=IMG_SIZE,         # Size of input images (28 for MNIST, 32 for CIFAR-10)\n",
        "    down_chs=(32, 64, 128),    # Channel dimensions for each downsampling level\n",
        "    t_embed_dim=8,             # Dimension for time step embeddings\n",
        "    c_embed_dim=N_CLASSES      # Number of classes for conditioning\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Input resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Input channels: {IMG_CH}\")\n",
        "print(f\"Time steps: {n_steps}\")\n",
        "print(f\"Condition classes: {N_CLASSES}\")\n",
        "print(f\"GPU acceleration: {'Yes' if device.type == 'cuda' else 'No'}\")\n",
        "\n",
        "# Validate model parameters and estimate memory requirements\n",
        "# Hint: Create functions to count parameters and estimate memory usage\n",
        "\n",
        "# Enter your code here:\n",
        "def count_parameters(model):\n",
        "    \"\"\"Counts the total number of trainable parameters in a model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total Trainable Parameters: {total_params:,} (~{total_params/1e6:.2f} M)\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    # Memory already allocated just for the model weights\n",
        "    allocated_mb = torch.cuda.memory_allocated(device) / (1024**2)\n",
        "    print(f\"Model VRAM (weights only): {allocated_mb:.2f} MB\")\n",
        "    print(\"Note: Total VRAM usage during training will be much higher due to gradients,\")\n",
        "    print(\"      optimizer states (Adam), and batch activations.\")\n",
        "\n",
        "\n",
        "# Your code to verify data ranges and integrity\n",
        "# Hint: Create functions to check data ranges in training and validation data\n",
        "\n",
        "# Enter your code here:\n",
        "def check_data_loader(loader, name):\n",
        "    \"\"\"Grabs one batch and prints its properties to check integrity.\"\"\"\n",
        "    print(f\"\\n--- Checking {name} ---\")\n",
        "    try:\n",
        "        # Get one batch and move it to the CPU for checking\n",
        "        images, labels = next(iter(loader))\n",
        "        images, labels = images.cpu(), labels.cpu()\n",
        "\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Image data type:   {images.dtype}\")\n",
        "        print(f\"  Image min/max/mean: {images.min():.2f} / {images.max():.2f} / {images.mean():.2f}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Label data type:   {labels.dtype}\")\n",
        "        print(f\"  Label min/max:     {labels.min()} / {labels.max()}\")\n",
        "        print(f\"  Image has NaNs:    {torch.isnan(images).any()}\")\n",
        "        print(f\"  Image has Infs:    {torch.isinf(images).any()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error checking {name}: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"DATA LOADER INTEGRITY CHECK\")\n",
        "print(f\"{'='*50}\")\n",
        "check_data_loader(train_loader, \"Training Loader\")\n",
        "check_data_loader(val_loader, \"Validation Loader\")\n",
        "print(\"\\nCheck: Image min/max should be approx. [-1.0, 1.0].\")\n",
        "print(\"Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\")\n",
        "\n",
        "\n",
        "# Set up the optimizer with parameters tuned for diffusion models\n",
        "# Note: Lower learning rates tend to work better for diffusion models\n",
        "initial_lr = 0.001  # Starting learning rate\n",
        "weight_decay = 1e-5  # L2 regularization to prevent overfitting\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=initial_lr,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
        "# This helps fine-tune the model toward the end of training\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',              # Reduce LR when monitored value stops decreasing\n",
        "    factor=0.5,              # Multiply LR by this factor\n",
        "    patience=5,              # Number of epochs with no improvement after which LR will be reduced\n",
        "    # verbose=True,          # <-- THIS LINE WAS REMOVED. It causes a TypeError in newer PyTorch.\n",
        "    min_lr=1e-6              # Lower bound on the learning rate\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\")\n",
        "\n",
        "# STUDENT EXPERIMENT:\n",
        "# Try different channel configurations and see how they affect:\n",
        "# 1. Model size (parameter count)\n",
        "# 2. Training time\n",
        "# 3. Generated image quality\n",
        "#\n",
        "# Suggestions:\n",
        "# - Smaller: down_chs=(16, 32, 64)\n",
        "# - Larger: down_chs=(64, 128, 256, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lGhYkj3-dtw",
        "outputId": "d581fb64-e708-46ca-e085-41ea24ff5289"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MODEL ARCHITECTURE SUMMARY\n",
            "==================================================\n",
            "Input resolution: 28x28\n",
            "Input channels: 1\n",
            "Time steps: 100\n",
            "Condition classes: 10\n",
            "GPU acceleration: Yes\n",
            "Total Trainable Parameters: 3,528,652 (~3.53 M)\n",
            "Model VRAM (weights only): 26.94 MB\n",
            "Note: Total VRAM usage during training will be much higher due to gradients,\n",
            "      optimizer states (Adam), and batch activations.\n",
            "\n",
            "==================================================\n",
            "DATA LOADER INTEGRITY CHECK\n",
            "==================================================\n",
            "\n",
            "--- Checking Training Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.73\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "--- Checking Validation Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.73\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "Check: Image min/max should be approx. [-1.0, 1.0].\n",
            "Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\n",
            "\n",
            "✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        if out_ch % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while out_ch % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if out_ch % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = 1\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "# 3. HELPER CLASS: DownBlock\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            RearrangePoolBlock(out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 4. HELPER CLASS: UpBlock\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            GELUConvBlock(2 * in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x_up = self.up(x)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.conv(x_cat)\n",
        "\n",
        "# 5. MAIN UNET CLASS (WITH THE FIX)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "        GS = 8 # Default Group Size\n",
        "        self.down_chs = down_chs\n",
        "        self.t_embed_dim = t_embed_dim\n",
        "        self.c_embed_dim = c_embed_dim\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # Class embedding (assumes N_CLASSES is globally defined)\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "\n",
        "        # Initial convolution\n",
        "        self.init_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Downsampling path\n",
        "        self.downs = nn.ModuleList()\n",
        "        for i in range(len(down_chs) - 1):\n",
        "            self.downs.append(\n",
        "                DownBlock(down_chs[i], down_chs[i+1], GS)\n",
        "            )\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mids = nn.Sequential(\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS),\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS)\n",
        "        )\n",
        "        self.mid_t_proj = nn.Linear(t_embed_dim, down_chs[-1])\n",
        "        self.mid_c_proj = nn.Linear(c_embed_dim, down_chs[-1])\n",
        "\n",
        "        # --- THIS IS THE FIXED SECTION ---\n",
        "        # Upsampling path\n",
        "        self.ups = nn.ModuleList()\n",
        "        # We loop from i = (e.g., 2) down to 1\n",
        "        for i in range(len(down_chs)-1, 0, -1):\n",
        "            # The UpBlock takes (in_chs, out_chs)\n",
        "            # e.g., in=128, out=64 --> (down_chs[2], down_chs[1])\n",
        "            # e.g., in=64,  out=32 --> (down_chs[1], down_chs[0])\n",
        "            self.ups.append(\n",
        "                UpBlock(down_chs[i], down_chs[i-1], GS) # Corrected: [i+1] -> [i], [i] -> [i-1]\n",
        "            )\n",
        "        # --- END OF FIX ---\n",
        "\n",
        "        # Final convolution\n",
        "        self.final_conv = nn.Conv2d(down_chs[0], img_ch, kernel_size=1)\n",
        "        print(f\"✅ Created UNet with {len(down_chs)} scale levels\")\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        \"\"\"\n",
        "        Forward pass through the UNet.\n",
        "        \"\"\"\n",
        "        t_embed = self.time_embed(t)\n",
        "        c_embed = self.class_embed(c)\n",
        "        c_embed = c_embed * c_mask # Apply mask\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        skips = []\n",
        "        for down_block in self.downs:\n",
        "            skips.append(x)\n",
        "            x = down_block(x)\n",
        "\n",
        "        x = self.mids(x)\n",
        "        b, c_dim, h_dim, w_dim = x.shape\n",
        "\n",
        "        t_proj = self.mid_t_proj(t_embed).view(b, c_dim, 1, 1)\n",
        "        c_proj = self.mid_c_proj(c_embed).view(b, c_dim, 1, 1)\n",
        "        x = x + t_proj + c_proj\n",
        "\n",
        "        # --- CORRECTION FOR FORWARD PASS ---\n",
        "        # We must iterate through skips in reverse (LIFO)\n",
        "        # and match them to the UpBlocks\n",
        "        for up_block in self.ups:\n",
        "            skip = skips.pop()\n",
        "            x = up_block(x, skip)\n",
        "        # --- END OF CORRECTION ---\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "print(\"✅ All model classes (UNet and helpers) are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoJzDavq-uSQ",
        "outputId": "52fa81b2-5169-4808-de99-570e7c5ad24f"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All model classes (UNet and helpers) are defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# from torch.optim import Adam\n",
        "# model, device, IMG_SIZE, IMG_CH, n_steps, N_CLASSES,\n",
        "# train_loader, val_loader, UNet\n",
        "# )\n",
        "from torch.optim import Adam # Added import for the optimizer\n",
        "\n",
        "# Create our model and move it to GPU if available\n",
        "# NOTE: Ensure you have run the cell defining the corrected UNet class\n",
        "model = UNet(\n",
        "    T=n_steps,                 # Number of diffusion time steps\n",
        "    img_ch=IMG_CH,             # Number of channels in our images (1 for grayscale, 3 for RGB)\n",
        "    img_size=IMG_SIZE,         # Size of input images (28 for MNIST, 32 for CIFAR-10)\n",
        "    down_chs=(32, 64, 128),    # Channel dimensions for each downsampling level\n",
        "    t_embed_dim=8,             # Dimension for time step embeddings\n",
        "    c_embed_dim=N_CLASSES      # Number of classes for conditioning\n",
        ").to(device)\n",
        "\n",
        "# Print model summary\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"MODEL ARCHITECTURE SUMMARY\")\n",
        "print(f\"{'='*50}\")\n",
        "print(f\"Input resolution: {IMG_SIZE}x{IMG_SIZE}\")\n",
        "print(f\"Input channels: {IMG_CH}\")\n",
        "print(f\"Time steps: {n_steps}\")\n",
        "print(f\"Condition classes: {N_CLASSES}\")\n",
        "print(f\"GPU acceleration: {'Yes' if device.type == 'cuda' else 'No'}\")\n",
        "\n",
        "# Validate model parameters and estimate memory requirements\n",
        "def count_parameters(model):\n",
        "    \"\"\"Counts the total number of trainable parameters in a model.\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "total_params = count_parameters(model)\n",
        "print(f\"Total Trainable Parameters: {total_params:,} (~{total_params/1e6:.2f} M)\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    # Memory already allocated just for the model weights\n",
        "    allocated_mb = torch.cuda.memory_allocated(device) / (1024**2)\n",
        "    print(f\"Model VRAM (weights only): {allocated_mb:.2f} MB\")\n",
        "    print(\"Note: Total VRAM usage during training will be much higher due to gradients,\")\n",
        "    print(\"      optimizer states (Adam), and batch activations.\")\n",
        "\n",
        "\n",
        "# Your code to verify data ranges and integrity\n",
        "def check_data_loader(loader, name):\n",
        "    \"\"\"Grabs one batch and prints its properties to check integrity.\"\"\"\n",
        "    print(f\"\\n--- Checking {name} ---\")\n",
        "    try:\n",
        "        # Get one batch and move it to the CPU for checking\n",
        "        images, labels = next(iter(loader))\n",
        "        images, labels = images.cpu(), labels.cpu()\n",
        "\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Image data type:   {images.dtype}\")\n",
        "        print(f\"  Image min/max/mean: {images.min():.2f} / {images.max():.2f} / {images.mean():.2f}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Label data type:   {labels.dtype}\")\n",
        "        print(f\"  Label min/max:     {labels.min()} / {labels.max()}\")\n",
        "        print(f\"  Image has NaNs:    {torch.isnan(images).any()}\")\n",
        "        print(f\"  Image has Infs:    {torch.isinf(images).any()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error checking {name}: {e}\")\n",
        "\n",
        "print(f\"\\n{'='*50}\")\n",
        "print(f\"DATA LOADER INTEGRITY CHECK\")\n",
        "print(f\"{'='*50}\")\n",
        "check_data_loader(train_loader, \"Training Loader\")\n",
        "check_data_loader(val_loader, \"Validation Loader\")\n",
        "print(\"\\nCheck: Image min/max should be approx. [-1.0, 1.0].\")\n",
        "print(\"Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\")\n",
        "\n",
        "\n",
        "# Set up the optimizer with parameters tuned for diffusion models\n",
        "initial_lr = 0.001  # Starting learning rate\n",
        "weight_decay = 1e-5  # L2 regularization to prevent overfitting\n",
        "\n",
        "optimizer = Adam(\n",
        "    model.parameters(),\n",
        "    lr=initial_lr,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "\n",
        "# Learning rate scheduler to reduce LR when validation loss plateaus\n",
        "# This helps fine-tune the model toward the end of training\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',              # Reduce LR when monitored value stops decreasing\n",
        "    factor=0.5,              # Multiply LR by this factor\n",
        "    patience=5,              # Number of epochs with no improvement after which LR will be reduced\n",
        "    # verbose=True,          # <-- THIS LINE WAS REMOVED TO FIX THE TypeError\n",
        "    min_lr=1e-6              # Lower bound on the learning rate\n",
        ")\n",
        "\n",
        "print(\"\\n✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\")\n",
        "\n",
        "# STUDENT EXPERIMENT:\n",
        "# Try different channel configurations and see how they affect:\n",
        "# 1. Model size (parameter count)\n",
        "# 2. Training time\n",
        "# 3. Generated image quality\n",
        "#\n",
        "# Suggestions:\n",
        "# - Smaller: down_chs=(16, 32, 64)\n",
        "# - Larger: down_chs=(64, 128, 256, 512)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnPKGcPf_FV1",
        "outputId": "66700b6d-0103-4a9f-fd6d-df41588a4895"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Created UNet with 3 scale levels\n",
            "\n",
            "==================================================\n",
            "MODEL ARCHITECTURE SUMMARY\n",
            "==================================================\n",
            "Input resolution: 28x28\n",
            "Input channels: 1\n",
            "Time steps: 100\n",
            "Condition classes: 10\n",
            "GPU acceleration: Yes\n",
            "Total Trainable Parameters: 3,841,773 (~3.84 M)\n",
            "Model VRAM (weights only): 28.14 MB\n",
            "Note: Total VRAM usage during training will be much higher due to gradients,\n",
            "      optimizer states (Adam), and batch activations.\n",
            "\n",
            "==================================================\n",
            "DATA LOADER INTEGRITY CHECK\n",
            "==================================================\n",
            "\n",
            "--- Checking Training Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.73\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "--- Checking Validation Loader ---\n",
            "  Image batch shape: torch.Size([64, 1, 28, 28])\n",
            "  Image data type:   torch.float32\n",
            "  Image min/max/mean: -1.00 / 1.00 / -0.73\n",
            "  Label batch shape: torch.Size([64])\n",
            "  Label data type:   torch.int64\n",
            "  Label min/max:     0 / 9\n",
            "  Image has NaNs:    False\n",
            "  Image has Infs:    False\n",
            "\n",
            "Check: Image min/max should be approx. [-1.0, 1.0].\n",
            "Check: Label min/max should be [0, 9] for MNIST/FashionMNIST.\n",
            "\n",
            "✅ Optimizer (Adam) and Scheduler (ReduceLROnPlateau) are set up.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes you have already run:\n",
        "# from einops.layers.torch import Rearrange\n",
        "# )\n",
        "\n",
        "# Rearranges pixels to downsample the image (2x reduction in spatial dimensions)\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        \"\"\"\n",
        "        Downsamples the spatial dimensions by 2x while preserving information\n",
        "\n",
        "        Args:\n",
        "            in_chs (int): Number of input channels\n",
        "            group_size (int): Number of groups for GroupNorm\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Your code to create the rearrange operation and convolution\n",
        "        # Hint: Use Rearrange from einops.layers.torch to reshape pixels\n",
        "        # Then add a GELUConvBlock to process the rearranged tensor\n",
        "\n",
        "        # Enter your code here:\n",
        "\n",
        "        # *** THIS IS THE FIXED LINE ***\n",
        "        # We use names 'p1' and 'p2' in the pattern string,\n",
        "        # and define their values (p1=2, p2=2) as arguments.\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "\n",
        "        # The number of input channels for the conv block is now 4 * in_chs\n",
        "        # (because p1*p2 = 4)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        # We need to make sure the group_size is valid for the new channel count\n",
        "        if new_chs % group_size != 0:\n",
        "            # Adjust group_size to be a divisor of new_chs\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: # Failsafe if it becomes 1\n",
        "                valid_group_size = new_chs\n",
        "            print(f\"RearrangePoolBlock adjusted group_size from {group_size} to {valid_group_size} for {new_chs} channels\")\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Your code for the forward pass\n",
        "        # Hint: Apply rearrange to downsample, then apply convolution\n",
        "\n",
        "        # Enter your code here:\n",
        "        # 1. Downsample by rearrangement\n",
        "        x = self.rearrange(x)\n",
        "        # 2. Process with convolution\n",
        "        x = self.conv_block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "XoDBRuD2Qkmt"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_loader(loader, name):\n",
        "    \"\"\"Grabs one batch and prints its properties to check integrity.\"\"\"\n",
        "    # THIS IS THE FIXED LINE:\n",
        "    print(f\"\\n--- Checking {name} ---\")\n",
        "    try:\n",
        "        images, labels = next(iter(loader))\n",
        "        print(f\"  Image batch shape: {images.shape}\")\n",
        "        print(f\"  Image data type:   {images.dtype}\")\n",
        "        print(f\"  Image min/max/mean: {images.min():.2f} / {images.max():.2f} / {images.mean():.2f}\")\n",
        "        print(f\"  Label batch shape: {labels.shape}\")\n",
        "        print(f\"  Label data type:   {labels.dtype}\")\n",
        "        print(f\"  Label min/max:     {labels.min()} / {labels.max()}\")\n",
        "        print(f\"  Image has NaNs:    {torch.isnan(images).any()}\")\n",
        "        print(f\"  Image has Infs:    {torch.isinf(images).any()}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error checking {name}: {e}\")"
      ],
      "metadata": {
        "id": "K3srkxXHJ1Af"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "Z_JVmioqCnQH"
      },
      "outputs": [],
      "source": [
        "# Define helper functions needed for training and evaluation\n",
        "def validate_model_parameters(model):\n",
        "    \"\"\"\n",
        "    Counts model parameters and estimates memory usage.\n",
        "    \"\"\"\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "\n",
        "    # Estimate memory requirements (very approximate)\n",
        "    param_memory = total_params * 4 / (1024 ** 2)  # MB for params (float32)\n",
        "    grad_memory = trainable_params * 4 / (1024 ** 2)  # MB for gradients\n",
        "    buffer_memory = param_memory * 2  # Optimizer state, forward activations, etc.\n",
        "\n",
        "    print(f\"Estimated GPU memory usage: {param_memory + grad_memory + buffer_memory:.1f} MB\")\n",
        "\n",
        "# Define helper functions for verifying data ranges\n",
        "def verify_data_range(dataloader, name=\"Dataset\"):\n",
        "    \"\"\"\n",
        "    Verifies the range and integrity of the data.\n",
        "    \"\"\"\n",
        "    batch = next(iter(dataloader))[0]\n",
        "    print(f\"\\n{name} range check:\")\n",
        "    print(f\"Shape: {batch.shape}\")\n",
        "    print(f\"Data type: {batch.dtype}\")\n",
        "    print(f\"Min value: {batch.min().item():.2f}\")\n",
        "    print(f\"Max value: {batch.max().item():.2f}\")\n",
        "    print(f\"Contains NaN: {torch.isnan(batch).any().item()}\")\n",
        "    print(f\"Contains Inf: {torch.isinf(batch).any().item()}\")\n",
        "\n",
        "# Define helper functions for generating samples during training\n",
        "def generate_samples(model, n_samples=10):\n",
        "    \"\"\"\n",
        "    Generates sample images using the model for visualization during training.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Generate digits 0-9 for visualization\n",
        "        samples = []\n",
        "        for digit in range(min(n_samples, 10)):\n",
        "            # Start with random noise\n",
        "            x = torch.randn(1, IMG_CH, IMG_SIZE, IMG_SIZE).to(device)\n",
        "\n",
        "            # Set up conditioning for the digit\n",
        "            c = torch.tensor([digit]).to(device)\n",
        "            c_one_hot = F.one_hot(c, N_CLASSES).float().to(device)\n",
        "            c_mask = torch.ones_like(c.unsqueeze(-1)).to(device)\n",
        "\n",
        "            # Remove noise step by step\n",
        "            for t in range(n_steps-1, -1, -1):\n",
        "                t_batch = torch.full((1,), t).to(device)\n",
        "                x = remove_noise(x, t_batch, model, c_one_hot, c_mask)\n",
        "\n",
        "            samples.append(x)\n",
        "\n",
        "        # Combine samples and display\n",
        "        samples = torch.cat(samples, dim=0)\n",
        "        grid = make_grid(samples, nrow=min(n_samples, 5), normalize=True)\n",
        "\n",
        "        plt.figure(figsize=(10, 4))\n",
        "\n",
        "        # Display based on channel configuration\n",
        "        if IMG_CH == 1:\n",
        "            plt.imshow(grid[0].cpu(), cmap='gray')\n",
        "        else:\n",
        "            plt.imshow(grid.permute(1, 2, 0).cpu())\n",
        "\n",
        "        plt.axis('off')\n",
        "        plt.title('Generated Samples')\n",
        "        plt.show()\n",
        "\n",
        "# Define helper functions for safely saving models\n",
        "def safe_save_model(model, path, optimizer=None, epoch=None, best_loss=None):\n",
        "    \"\"\"\n",
        "    Safely saves model with error handling and backup.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create a dictionary with all the elements to save\n",
        "        save_dict = {\n",
        "            'model_state_dict': model.state_dict(),\n",
        "        }\n",
        "\n",
        "        # Add optional elements if provided\n",
        "        if optimizer is not None:\n",
        "            save_dict['optimizer_state_dict'] = optimizer.state_dict()\n",
        "        if epoch is not None:\n",
        "            save_dict['epoch'] = epoch\n",
        "        if best_loss is not None:\n",
        "            save_dict['best_loss'] = best_loss\n",
        "\n",
        "        # Create a backup of previous checkpoint if it exists\n",
        "        if os.path.exists(path):\n",
        "            backup_path = path + '.backup'\n",
        "            try:\n",
        "                os.replace(path, backup_path)\n",
        "                print(f\"Created backup at {backup_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Could not create backup - {e}\")\n",
        "\n",
        "        # Save the new checkpoint\n",
        "        torch.save(save_dict, path)\n",
        "        print(f\"Model successfully saved to {path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving model: {e}\")\n",
        "        print(\"Attempting emergency save...\")\n",
        "\n",
        "        try:\n",
        "            emergency_path = path + '.emergency'\n",
        "            torch.save(model.state_dict(), emergency_path)\n",
        "            print(f\"Emergency save successful: {emergency_path}\")\n",
        "        except:\n",
        "            print(\"Emergency save failed. Could not save model.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "GVltoIZyL3US"
      },
      "outputs": [],
      "source": [
        "#  Implementation of the training step function\n",
        "def train_step(x, c):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the diffusion model.\n",
        "\n",
        "    This function:\n",
        "    1. Prepares class conditioning\n",
        "    2. Samples random timesteps for each image\n",
        "    3. Adds corresponding noise to the images\n",
        "    4. Asks the model to predict the noise\n",
        "    5. Calculates the loss between predicted and actual noise\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Batch of clean images [batch_size, channels, height, width]\n",
        "        c (torch.Tensor): Batch of class labels [batch_size]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Mean squared error loss value\n",
        "    \"\"\"\n",
        "    # Convert number labels to one-hot encoding for class conditioning\n",
        "    # Example: Label 3 -> [0, 0, 0, 1, 0, 0, 0, 0, 0, 0] for MNIST\n",
        "    c_one_hot = F.one_hot(c, N_CLASSES).float().to(device)\n",
        "\n",
        "    # Create conditioning mask (all ones for standard training)\n",
        "    # This would be used for classifier-free guidance if implemented\n",
        "    c_mask = torch.ones_like(c.unsqueeze(-1)).to(device)\n",
        "\n",
        "    # Pick random timesteps for each image in the batch\n",
        "    # Different timesteps allow the model to learn the entire diffusion process\n",
        "    t = torch.randint(0, n_steps, (x.shape[0],)).to(device)\n",
        "\n",
        "    # Add noise to images according to the forward diffusion process\n",
        "    # This simulates images at different stages of the diffusion process\n",
        "    # Hint: Use the add_noise function you defined earlier\n",
        "\n",
        "    # Enter your code here:\n",
        "\n",
        "    # The model tries to predict the exact noise that was added\n",
        "    # This is the core learning objective of diffusion models\n",
        "    predicted_noise = model(x_t, t, c_one_hot, c_mask)\n",
        "\n",
        "    # Calculate loss: how accurately did the model predict the noise?\n",
        "    # MSE loss works well for image-based diffusion models\n",
        "    # Hint: Use F.mse_loss to compare predicted and actual noise\n",
        "\n",
        "    # Enter your code here:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F # Make sure F is imported\n",
        "\n",
        "#  Implementation of the training step function\n",
        "def train_step(x, c):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the diffusion model.\n",
        "\n",
        "    This function:\n",
        "    1. Prepares class conditioning\n",
        "    2. Samples random timesteps for each image\n",
        "    3. Adds corresponding noise to the images\n",
        "    4. Asks the model to predict the noise\n",
        "    5. Calculates the loss between predicted and actual noise\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Batch of clean images [batch_size, channels, height, width]\n",
        "        c (torch.Tensor): Batch of class labels [batch_size]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Mean squared error loss value\n",
        "    \"\"\"\n",
        "\n",
        "    # --- CORRECTION ---\n",
        "    # Move data to the GPU first.\n",
        "    # The UNet model we built takes class INDICES (c), not one-hot vectors.\n",
        "    x = x.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    # We do NOT need this line:\n",
        "    # c_one_hot = F.one_hot(c, N_CLASSES).float().to(device)\n",
        "\n",
        "    # Create conditioning mask (all ones for standard training)\n",
        "    # This is used for classifier-free guidance (if implemented)\n",
        "    c_mask = torch.ones(c.shape[0], 1).to(device) # Shape [B, 1]\n",
        "\n",
        "    # Pick random timesteps for each image in the batch\n",
        "    # Different timesteps allow the model to learn the entire diffusion process\n",
        "    t = torch.randint(0, n_steps, (x.shape[0],)).to(device)\n",
        "\n",
        "    # Add noise to images according to the forward diffusion process\n",
        "    # This simulates images at different stages of the diffusion process\n",
        "    # Hint: Use the add_noise function you defined earlier\n",
        "\n",
        "    # Enter your code here:\n",
        "    x_t, actual_noise = add_noise(x, t)\n",
        "\n",
        "    # The model tries to predict the exact noise that was added\n",
        "    # This is the core learning objective of diffusion models\n",
        "    # We pass 'c' (indices) to the model, not 'c_one_hot'\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "\n",
        "    # Calculate loss: how accurately did the model predict the noise?\n",
        "    # MSE loss works well for image-based diffusion models\n",
        "    # Hint: Use F.mse_loss to compare predicted and actual noise\n",
        "\n",
        "    # Enter your code here:\n",
        "    loss = F.mse_loss(predicted_noise, actual_noise)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "qq_rLa4c4kt3"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F # Make sure F is imported\n",
        "\n",
        "#  Implementation of the training step function\n",
        "def train_step(x, c):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the diffusion model.\n",
        "\n",
        "    This function:\n",
        "    1. Prepares class conditioning\n",
        "    2. Samples random timesteps for each image\n",
        "    3. Adds corresponding noise to the images\n",
        "    4. Asks the model to predict the noise\n",
        "    5. Calculates the loss between predicted and actual noise\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Batch of clean images [batch_size, channels, height, width]\n",
        "        c (torch.Tensor): Batch of class labels [batch_size]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Mean squared error loss value\n",
        "    \"\"\"\n",
        "\n",
        "    # *** CORRECTION ***\n",
        "    # Move data to the GPU first.\n",
        "    # The UNet model we built takes class INDICES (c), not one-hot vectors.\n",
        "    x = x.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    # We do NOT need this line:\n",
        "    # c_one_hot = F.one_hot(c, N_CLASSES).float().to(device)\n",
        "\n",
        "    # Create conditioning mask (all ones for standard training)\n",
        "    # This is used for classifier-free guidance (if implemented)\n",
        "    c_mask = torch.ones(c.shape[0], 1).to(device) # Shape [B, 1]\n",
        "\n",
        "    # Pick random timesteps for each image in the batch\n",
        "    # Different timesteps allow the model to learn the entire diffusion process\n",
        "    t = torch.randint(0, n_steps, (x.shape[0],)).to(device)\n",
        "\n",
        "    # Add noise to images according to the forward diffusion process\n",
        "    # This simulates images at different stages of the diffusion process\n",
        "    # Hint: Use the add_noise function you defined earlier\n",
        "\n",
        "    # Enter your code here:\n",
        "    x_t, actual_noise = add_noise(x, t)\n",
        "\n",
        "    # The model tries to predict the exact noise that was added\n",
        "    # This is the core learning objective of diffusion models\n",
        "    # We pass 'c' (indices) to the model, not 'c_one_hot'\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "\n",
        "    # Calculate loss: how accurately did the model predict the noise?\n",
        "    # MSE loss works well for image-based diffusion models\n",
        "    # Hint: Use F.mse_loss to compare predicted and actual noise\n",
        "\n",
        "    # Enter your code here:\n",
        "    loss = F.mse_loss(predicted_noise, actual_noise)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "azVQLMe5_fFK"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F # Make sure F is imported\n",
        "\n",
        "#  Implementation of the training step function\n",
        "def train_step(x, c):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the diffusion model.\n",
        "\n",
        "    This function:\n",
        "    1. Prepares class conditioning\n",
        "    2. Samples random timesteps for each image\n",
        "    3. Adds corresponding noise to the images\n",
        "    4. Asks the model to predict the noise\n",
        "    5. Calculates the loss between predicted and actual noise\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Batch of clean images [batch_size, channels, height, width]\n",
        "        c (torch.Tensor): Batch of class labels [batch_size]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Mean squared error loss value\n",
        "    \"\"\"\n",
        "\n",
        "    # We also need to move the original inputs to the device\n",
        "    x = x.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "\n",
        "    # Create conditioning mask (all ones for standard training)\n",
        "    # This would be used for classifier-free guidance if implemented\n",
        "    c_mask = torch.ones(c.shape[0], 1).to(device) # [B, 1]\n",
        "\n",
        "    # Pick random timesteps for each image in the batch\n",
        "    # Different timesteps allow the model to learn the entire diffusion process\n",
        "    t = torch.randint(0, n_steps, (x.shape[0],)).to(device)\n",
        "\n",
        "    # Add noise to images according to the forward diffusion process\n",
        "    # This simulates images at different stages of the diffusion process\n",
        "    # Hint: Use the add_noise function you defined earlier\n",
        "\n",
        "    # Enter your code here:\n",
        "    # --- THIS IS THE FIXED LINE ---\n",
        "    x_t, actual_noise = add_noise(x, t)\n",
        "\n",
        "    # The model tries to predict the exact noise that was added\n",
        "    # This is the core learning objective of diffusion models\n",
        "    # We pass 'c' (indices) to the model, not 'c_one_hot'\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "\n",
        "    # Calculate loss: how accurately did the model predict the noise?\n",
        "    # MSE loss works well for image-based diffusion models\n",
        "    # Hint: Use F.mse_loss to compare predicted and actual noise\n",
        "\n",
        "    # Enter your code here:\n",
        "    # --- THIS IS THE OTHER FILLED-IN LINE ---\n",
        "    loss = F.mse_loss(predicted_noise, actual_noise)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "InwgSdSw_v8a"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# n_steps = ... (e.g., 300)\n",
        "# device = ... (e.g., 'cuda')\n",
        "# N_CLASSES = ... (e.g., 10)\n",
        "# add_noise = ... (the function you defined)\n",
        "# model = ... (the UNet model you defined)\n",
        "# )\n",
        "import torch.nn.functional as F # Make sure F is imported\n",
        "\n",
        "#  Implementation of the training step function\n",
        "def train_step(x, c):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the diffusion model.\n",
        "\n",
        "    This function:\n",
        "    1. Prepares class conditioning\n",
        "    2. Samples random timesteps for each image\n",
        "    3. Adds corresponding noise to the images\n",
        "    4. Asks the model to predict the noise\n",
        "    5. Calculates the loss between predicted and actual noise\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Batch of clean images [batch_size, channels, height, width]\n",
        "        c (torch.Tensor): Batch of class labels [batch_size]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Mean squared error loss value\n",
        "    \"\"\"\n",
        "\n",
        "    # *** CORRECTION ***\n",
        "    # The UNet model we built uses nn.Embedding, which takes class INDICES.\n",
        "    # We do NOT need one-hot encoding.\n",
        "    # c_one_hot = F.one_hot(c, N_CLASSES).float().to(device) # <- This is not needed\n",
        "\n",
        "    # Move data to the GPU\n",
        "    x = x.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "    # Create conditioning mask (all ones for standard training)\n",
        "    c_mask = torch.ones(c.shape[0], 1).to(device) # [B, 1]\n",
        "\n",
        "    # Pick random timesteps for each image in the batch\n",
        "    t = torch.randint(0, n_steps, (x.shape[0],)).to(device)\n",
        "\n",
        "    # Add noise to images according to the forward diffusion process\n",
        "    # Hint: Use the add_noise function you defined earlier\n",
        "\n",
        "    # Enter your code here:\n",
        "    x_t, actual_noise = add_noise(x, t)\n",
        "\n",
        "\n",
        "    # The model tries to predict the exact noise that was added\n",
        "    # We pass 'c' (indices) to the model, not 'c_one_hot'\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "\n",
        "    # Calculate loss: how accurately did the model predict the noise?\n",
        "    # Hint: Use F.mse_loss to compare predicted and actual noise\n",
        "\n",
        "    # Enter your code here:\n",
        "    loss = F.mse_loss(predicted_noise, actual_noise)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "ZH2mk-U8Q3fY"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F # Make sure F is imported\n",
        "\n",
        "#  Implementation of the training step function\n",
        "def train_step(x, c):\n",
        "    \"\"\"\n",
        "    Performs a single training step for the diffusion model.\n",
        "\n",
        "    This function:\n",
        "    1. Prepares class conditioning\n",
        "    2. Samples random timesteps for each image\n",
        "    3. Adds corresponding noise to the images\n",
        "    4. Asks the model to predict the noise\n",
        "    5. Calculates the loss between predicted and actual noise\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Batch of clean images [batch_size, channels, height, width]\n",
        "        c (torch.Tensor): Batch of class labels [batch_size]\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Mean squared error loss value\n",
        "    \"\"\"\n",
        "\n",
        "    # Move the original inputs to the device\n",
        "    x = x.to(device)\n",
        "    c = c.to(device)\n",
        "\n",
        "\n",
        "    # Create conditioning mask (all ones for standard training)\n",
        "    c_mask = torch.ones(c.shape[0], 1).to(device) # [B, 1]\n",
        "\n",
        "    # Pick random timesteps for each image in the batch\n",
        "    t = torch.randint(0, n_steps, (x.shape[0],)).to(device)\n",
        "\n",
        "    # Add noise to images according to the forward diffusion process\n",
        "    # Hint: Use the add_noise function you defined earlier\n",
        "\n",
        "    # THIS IS THE FIXED LINE:\n",
        "    x_t, actual_noise = add_noise(x, t)\n",
        "\n",
        "    # The model tries to predict the exact noise that was added\n",
        "    # We pass 'c' (indices) to the model\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "\n",
        "    # Calculate loss: how accurately did the model predict the noise?\n",
        "    # Hint: Use F.mse_loss to compare predicted and actual noise\n",
        "\n",
        "    # Enter your code here:\n",
        "    loss = F.mse_loss(predicted_noise, actual_noise)\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "akG9ZwgwKani"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# import traceback\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist,\n",
        "#  but they are commented out below to prevent errors if not defined yet)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "# The ENTIRE block from 'try' to 'finally' must be in one cell\n",
        "try:\n",
        "    # The 'for' loop is now correctly indented inside the 'try' block\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        # FIXED: dataloader -> train_loader\n",
        "        for step, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            # FIXED: dataloader -> val_loader\n",
        "            for val_images, val_labels in val_loader:\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels)\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # Use safe_save_model instead of just saving state_dict\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, avg_val_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "opGaWkTu6WTi",
        "outputId": "2b94ce2b-bd4a-4064-b3a6-50a2957037ea"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-1932904134.py\", line 51, in <cell line: 0>\n",
            "    loss = train_step(images, labels)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_step(x, c):\n",
        "    # Move data to the GPU\n",
        "    x = x.to(device)\n",
        "    c = c.to(device) # 'c' is already indices, e.g., [1, 5, 0] (type Long)\n",
        "\n",
        "    # Create conditioning mask\n",
        "    c_mask = torch.ones(c.shape[0], 1).to(device)\n",
        "\n",
        "    # Pick random timesteps\n",
        "    t = torch.randint(0, n_steps, (x.shape[0],)).to(device)\n",
        "\n",
        "    # Add noise\n",
        "    x_t, actual_noise = add_noise(x, t)\n",
        "\n",
        "    # Predict noise\n",
        "    # FIX: Pass 'c' (indices), not 'c_one_hot'\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "\n",
        "    # Calculate loss\n",
        "    loss = F.mse_loss(predicted_noise, actual_noise)\n",
        "\n",
        "    return loss\n",
        "\n",
        "print(\"✅ 'train_step' function is defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jjLW4VT6B2H",
        "outputId": "a8396fc6-d8ce-4ed1-a428-be58485782de"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 'train_step' function is defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock (No changes)\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        if out_ch % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while out_ch % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if out_ch % valid_group_size != 0: valid_group_size = 1\n",
        "            group_size = valid_group_size\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock (FIXED)\n",
        "# Takes in_chs and outputs out_chs to fix channel mismatch\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        # Fix group_size for new_chs\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        # This conv now correctly maps 4*in_chs -> out_chs\n",
        "        self.conv_block = GELUConvBlock(new_chs, out_chs, group_size)\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "# 3. HELPER CLASS: DownBlock (FIXED)\n",
        "# Passes the correct out_chs to the RearrangePoolBlock\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            # This now correctly takes 'out_chs' and outputs 'out_chs'\n",
        "            RearrangePoolBlock(out_chs, out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 4. HELPER CLASS: UpBlock (FIXED)\n",
        "# Correctly handles different channels from skip connection\n",
        "class UpBlock(nn.Module):\n",
        "    # Takes in_chs (from below), skip_chs (from skip), and out_chs\n",
        "    def __init__(self, in_chs, skip_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "        # Conv block now takes (in_chs + skip_chs)\n",
        "        self.conv = nn.Sequential(\n",
        "            GELUConvBlock(in_chs + skip_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x_up = self.up(x)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.conv(x_cat)\n",
        "\n",
        "# 5. MAIN UNET CLASS (FIXED)\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super().__init__()\n",
        "        GS = 8\n",
        "        self.down_chs = down_chs\n",
        "        self.t_embed_dim = t_embed_dim\n",
        "        self.c_embed_dim = c_embed_dim\n",
        "\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Embedding(T, t_embed_dim),\n",
        "            nn.Linear(t_embed_dim, t_embed_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        # FIX: Takes class indices (Long), not one-hot (Float)\n",
        "        self.class_embed = nn.Embedding(N_CLASSES, c_embed_dim)\n",
        "        self.init_conv = GELUConvBlock(img_ch, down_chs[0], GS)\n",
        "\n",
        "        # Downsampling path (channel logic is now correct)\n",
        "        self.downs = nn.ModuleList()\n",
        "        for i in range(len(down_chs) - 1):\n",
        "            self.downs.append(\n",
        "                DownBlock(down_chs[i], down_chs[i+1], GS)\n",
        "            )\n",
        "\n",
        "        # Middle blocks\n",
        "        self.mids = nn.Sequential(\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS),\n",
        "            GELUConvBlock(down_chs[-1], down_chs[-1], GS)\n",
        "        )\n",
        "        self.mid_t_proj = nn.Linear(t_embed_dim, down_chs[-1])\n",
        "        self.mid_c_proj = nn.Linear(c_embed_dim, down_chs[-1])\n",
        "\n",
        "        # Upsampling path (FIXED SIGNATURE)\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i in range(len(down_chs)-1, 0, -1):\n",
        "            # UpBlock(in_chs, skip_chs, out_chs)\n",
        "            self.ups.append(\n",
        "                UpBlock(down_chs[i], down_chs[i-1], down_chs[i-1], GS)\n",
        "            )\n",
        "\n",
        "        self.final_conv = nn.Conv2d(down_chs[0], img_ch, kernel_size=1)\n",
        "        print(f\"✅ Created UNet with {len(down_chs)} scale levels\")\n",
        "\n",
        "    def forward(self, x, t, c, c_mask):\n",
        "        # FIX: 'c' is now expected to be class INDICES (type Long)\n",
        "        t_embed = self.time_embed(t)\n",
        "        c_embed = self.class_embed(c) # This now works\n",
        "        c_embed = c_embed * c_mask\n",
        "        x = self.init_conv(x)\n",
        "\n",
        "        skips = []\n",
        "        for down_block in self.downs:\n",
        "            skips.append(x)\n",
        "            x = down_block(x)\n",
        "\n",
        "        x = self.mids(x)\n",
        "        b, c_dim, h_dim, w_dim = x.shape\n",
        "        t_proj = self.mid_t_proj(t_embed).view(b, c_dim, 1, 1)\n",
        "        c_proj = self.mid_c_proj(c_embed).view(b, c_dim, 1, 1)\n",
        "        x = x + t_proj + c_proj\n",
        "\n",
        "        for up_block in self.ups:\n",
        "            skip = skips.pop()\n",
        "            x = up_block(x, skip)\n",
        "\n",
        "        return self.final_conv(x)\n",
        "\n",
        "print(\"✅ All model classes (UNet and helpers) are defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMLyhP7S5zm_",
        "outputId": "f37b7005-629a-459d-fd0d-fe4f1d540e1a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All model classes (UNet and helpers) are defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# import traceback\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist,\n",
        "#  but they are commented out below to prevent errors if not defined yet)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "# The ENTIRE block from 'try' to 'finally' must be in one cell\n",
        "try:\n",
        "    # The 'for' loop is no longer indented and is inside the 'try' block.\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Fixed: dataloader -> train_loader\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Fixed: dataloader -> val_loader\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels)\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, avg_val_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "aJDTenVgAh0h",
        "outputId": "51c9cc54-026a-4162-ca06-6a3212f492d1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2523089668.py\", line 50, in <cell line: 0>\n",
            "    loss = train_step(images, labels)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels)\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # Use avg_val_loss or last epoch loss for saving\n",
        "    last_loss = val_losses[-1] if val_losses else avg_train_loss\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, last_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-V8g2DanK3tS",
        "outputId": "77b94235-0f49-41ac-888b-3cfef37ef0c3"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-999849933.py\", line 47, in <cell line: 0>\n",
            "    loss = train_step(images, labels)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# import traceback\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist,\n",
        "#  but they are commented out below to prevent errors if not defined yet)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        # THIS IS THE FIXED LINE:\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels)\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, avg_val_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "heLFvMU0RJvG",
        "outputId": "31272c64-c478-488f-b3f0-c31a86b60266"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2266080732.py\", line 49, in <cell line: 0>\n",
            "    loss = train_step(images, labels)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels)\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # Use avg_val_loss or last epoch loss for saving\n",
        "    last_loss = val_losses[-1] if val_losses else avg_train_loss\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, last_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bEIpnVRwLBgj",
        "outputId": "047dfe80-8cae-4913-ddf7-d2c6d54375e6"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-999849933.py\", line 47, in <cell line: 0>\n",
            "    loss = train_step(images, labels)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels)\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            # THIS IS THE FIXED LINE:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, avg_val_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "91l5seAFLL5g",
        "outputId": "e3d90427-33f3-4fe8-833e-992badda590a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-2283485391.py\", line 47, in <cell line: 0>\n",
            "    loss = train_step(images, labels)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "btbIyU4sMluk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "ecf3c1c8-2ca3-4910-9b4e-aa83f46b501b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Statistics:\n",
            "------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Plot training progress\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot training and validation losses for comparison\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "if len(val_losses) > 0:  # Only plot validation if it exists\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "\n",
        "# Improve the plot with better labels and styling\n",
        "plt.title('Diffusion Model Training Progress')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Add annotations for key points - only if lists are not empty\n",
        "if train_losses:\n",
        "    min_train_loss = min(train_losses)\n",
        "    min_train_idx = train_losses.index(min_train_loss)\n",
        "    if len(train_losses) > 0: # Ensure there's at least one point\n",
        "        plt.annotate(f'Min: {min_train_loss:.4f}',\n",
        "                     xy=(min_train_idx, min_train_loss),\n",
        "                     xytext=(min_train_idx, min_train_loss * 1.2 if min_train_loss > 0 else min_train_loss + 0.1),\n",
        "                     arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "                     fontsize=9)\n",
        "\n",
        "# Add validation min point if available\n",
        "if val_losses:\n",
        "    min_val_loss = min(val_losses)\n",
        "    min_val_idx = val_losses.index(min_val_loss)\n",
        "    if len(val_losses) > 0: # Ensure there's at least one point\n",
        "        plt.annotate(f'Min: {min_val_loss:.4f}',\n",
        "                    xy=(min_val_idx, min_val_loss),\n",
        "                    xytext=(min_val_idx, min_val_loss * 0.8 if min_val_loss > 0 else min_val_loss - 0.1),\n",
        "                    arrowprops=dict(facecolor='black', shrink=0.05),\n",
        "                    fontsize=9)\n",
        "\n",
        "# Set y-axis to start from 0 or slightly lower than min value\n",
        "# Handle cases where lists are empty or contain only inf (if training failed early)\n",
        "all_min_losses = [min(train_losses) if train_losses else float('inf'),\n",
        "                  min(val_losses) if val_losses else float('inf')]\n",
        "valid_min_losses = [loss for loss in all_min_losses if loss != float('inf')]\n",
        "\n",
        "if valid_min_losses:\n",
        "    min_overall_loss = min(valid_min_losses)\n",
        "    plt.ylim(bottom=max(0, min_overall_loss * 0.9))\n",
        "else:\n",
        "     # If no valid losses, set a default y-limit or let matplotlib auto-scale\n",
        "     plt.ylim(bottom=0) # Set bottom to 0 if no valid losses\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Add statistics summary for students to analyze\n",
        "print(\"\\nTraining Statistics:\")\n",
        "print(\"-\" * 30)\n",
        "if train_losses:\n",
        "    print(f\"Starting training loss:    {train_losses[0]:.4f}\")\n",
        "    print(f\"Final training loss:       {train_losses[-1]:.4f}\")\n",
        "    print(f\"Best training loss:        {min(train_losses):.4f}\")\n",
        "    if len(train_losses) > 1:\n",
        "         print(f\"Training loss improvement: {((train_losses[0] - min(train_losses)) / train_losses[0] * 100):.1f}%\")\n",
        "\n",
        "\n",
        "if val_losses:\n",
        "    print(\"\\nValidation Statistics:\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Starting validation loss: {val_losses[0]:.4f}\")\n",
        "    print(f\"Final validation loss:    {val_losses[-1]:.4f}\")\n",
        "    print(f\"Best validation loss:     {min(val_losses):.4f}\")\n",
        "\n",
        "# STUDENT EXERCISE:\n",
        "# 1. Try modifying this plot to show a smoothed version of the losses\n",
        "# 2. Create a second plot showing the ratio of validation to training loss\n",
        "#    (which can indicate overfitting when the ratio increases)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "==================================================\n",
        "STARTING TRAINING\n",
        "==================================================\n",
        "\n",
        "Epoch 1/30\n",
        "--------------------\n",
        "\n",
        "==================================================\n",
        "AN ERROR OCCURRED: Module [UNet] is missing the required \"forward\" function\n",
        "==================================================\n",
        "\n",
        "==================================================\n",
        "TRAINING COMPLETE\n",
        "==================================================\n",
        "Best validation loss: inf\n",
        "Generating final samples...\n",
        "Traceback (most recent call last):\n",
        "  File \"/tmp/ipython-input-2283485391.py\", line 47, in <cell line: 0>\n",
        "    loss = train_step(images, labels)\n",
        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "  File \"/tmp/ipython-input-3691158498.py\", line 42, in train_step\n",
        "    predicted_noise = model(x_t, t, c, c_mask)\n",
        "                     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
        "    return self._call_impl(*args, **kwargs)\n",
        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
        "    return forward_call(*args, **kwargs)\n",
        "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
        "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 399, in _forward_unimplemented\n",
        "    raise NotImplementedError(\n",
        "NotImplementedError: Module [UNet] is missing the required \"forward\" function\n",
        "Cleaning up CUDA cache...\n",
        "Done.\n"
      ],
      "metadata": {
        "id": "z522nUyfNq5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist,\n",
        "#  but they are commented out below to prevent errors if not defined yet)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels) # Pass both images and labels\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels) # Pass both images and labels\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # Use avg_val_loss or last epoch loss for saving\n",
        "    last_loss = val_losses[-1] if val_losses else avg_train_loss\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, last_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tiGPv100N-zY",
        "outputId": "facb38cf-9350-44a9-ae85-eaf4e17ffa36"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-915812843.py\", line 48, in <cell line: 0>\n",
            "    loss = train_step(images, labels) # Pass both images and labels\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist,\n",
        "#  but they are commented out below to prevent errors if not defined yet)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels) # Pass both images and labels\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    # generate_samples(model, n_samples=5) # Assumes this function exists\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels)\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            # safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Assumes this function exists\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # Use avg_val_loss or last epoch loss for saving\n",
        "    last_loss = val_losses[-1] if val_losses else avg_train_loss\n",
        "    # safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, last_loss) # Assumes this function exists\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    # generate_samples(model, n_samples=10) # Assumes this function exists\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TDLNR5GROO6h",
        "outputId": "348a7f4a-4590-434c-daa2-72a9b1c4fbb1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3223143959.py\", line 48, in <cell line: 0>\n",
            "    loss = train_step(images, labels) # Pass both images and labels\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up CUDA cache...\n",
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# import traceback # Import traceback for error handling\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist,\n",
        "#  and they are called below)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels) # Pass both images and labels\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    generate_samples(model, n_samples=5) # Call generate_samples\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels) # Pass both images and labels\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            generate_samples(model, n_samples=10) # Call generate_samples\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Call safe_save_model\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # Use avg_val_loss or last epoch loss for saving\n",
        "    last_loss = val_losses[-1] if val_losses else avg_train_loss\n",
        "    safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, last_loss) # Call safe_save_model with appropriate loss\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    generate_samples(model, n_samples=10) # Call generate_samples\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u4lyiqQcOXE5",
        "outputId": "23e78f56-4c46-45d0-9fd0-a982eda10a4a"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "STARTING TRAINING\n",
            "==================================================\n",
            "\n",
            "Epoch 1/30\n",
            "--------------------\n",
            "\n",
            "==================================================\n",
            "AN ERROR OCCURRED: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "TRAINING COMPLETE\n",
            "==================================================\n",
            "Best validation loss: inf\n",
            "Generating final samples...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-15724889.py\", line 49, in <cell line: 0>\n",
            "    loss = train_step(images, labels) # Pass both images and labels\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3113446762.py\", line 19, in train_step\n",
            "    predicted_noise = model(x_t, t, c, c_mask)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[64, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15724889.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# Generate final samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Generating final samples...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call generate_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;31m# Display final loss curves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1604396291.py\u001b[0m in \u001b[0;36mgenerate_samples\u001b[0;34m(model, n_samples)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mt_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_one_hot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2704042813.py\u001b[0m in \u001b[0;36mremove_noise\u001b[0;34m(x_t, t, model, c, c_mask)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \"\"\"\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Predict the noise in the image using our model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Get noise schedule values for the current timestep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2945402280.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t, c, c_mask)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m    132\u001b[0m         \u001b[0mt_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mc_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mc_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc_embed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mc_mask\u001b[0m \u001b[0;31m# Apply mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2544\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2545\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2546\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        if out_ch % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while out_ch % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if out_ch % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = 1\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        # Use named parameters (p1=2, p2=2) to fix the EinopsError\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "c7Rr_COYTO-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (This assumes the following are defined:\n",
        "# import torch\n",
        "# import matplotlib.pyplot as plt\n",
        "# import traceback # Import traceback for error handling\n",
        "# model, device, EPOCHS, train_loader, val_loader\n",
        "# optimizer, scheduler, train_step\n",
        "# n_steps, early_stopping_patience, gradient_clip_value,\n",
        "# display_frequency, generate_frequency\n",
        "# )\n",
        "# (It also assumes functions 'generate_samples' and 'safe_save_model' exist,\n",
        "#  and they are called below)\n",
        "\n",
        "# Implementation of the main training loop\n",
        "# Training configuration\n",
        "early_stopping_patience = 10  # Number of epochs without improvement before stopping\n",
        "gradient_clip_value = 1.0     # Maximum gradient norm for stability\n",
        "display_frequency = 100       # How often to show progress (in steps)\n",
        "generate_frequency = 500      # How often to generate samples (in steps)\n",
        "\n",
        "# Progress tracking variables\n",
        "best_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "no_improve_epochs = 0\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Wrap the training loop in a try-except block for better error handling\n",
        "try:\n",
        "    # This loop starts at the correct (zero) indentation level\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "        print(\"-\" * 20)\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        epoch_losses = []\n",
        "\n",
        "        # Process each batch\n",
        "        for step, (images, labels) in enumerate(train_loader):  # Using 'train_loader'\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Training step\n",
        "            optimizer.zero_grad()\n",
        "            loss = train_step(images, labels) # Pass both images and labels\n",
        "            loss.backward()\n",
        "\n",
        "            # Add gradient clipping for stability\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_value)\n",
        "\n",
        "            optimizer.step()\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "            # Show progress at regular intervals\n",
        "            if step % display_frequency == 0:\n",
        "                print(f\"  Step {step}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "                # Generate samples less frequently to save time\n",
        "                if step % generate_frequency == 0 and step > 0:\n",
        "                    print(\"  Generating samples...\")\n",
        "                    generate_samples(model, n_samples=5) # Call generate_samples\n",
        "\n",
        "        # End of epoch - calculate average training loss\n",
        "        avg_train_loss = sum(epoch_losses) / len(epoch_losses)\n",
        "        train_losses.append(avg_train_loss)\n",
        "        print(f\"\\nTraining - Epoch {epoch+1} average loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_epoch_losses = []\n",
        "        print(\"Running validation...\")\n",
        "\n",
        "        with torch.no_grad():  # Disable gradients for validation\n",
        "            for val_images, val_labels in val_loader: # Using 'val_loader'\n",
        "                val_images = val_images.to(device)\n",
        "                val_labels = val_labels.to(device)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                val_loss = train_step(val_images, val_labels) # Pass both images and labels\n",
        "                val_epoch_losses.append(val_loss.item())\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = sum(val_epoch_losses) / len(val_epoch_losses)\n",
        "        val_losses.append(avg_val_loss)\n",
        "        print(f\"Validation - Epoch {epoch+1} average loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "        # Learning rate scheduling based on validation loss\n",
        "        scheduler.step(avg_val_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"Learning rate: {current_lr:.6f}\")\n",
        "\n",
        "        # Generate samples at the end of each epoch\n",
        "        if epoch % 2 == 0 or epoch == EPOCHS - 1:\n",
        "            print(\"\\nGenerating samples for visual progress check...\")\n",
        "            generate_samples(model, n_samples=10) # Call generate_samples\n",
        "\n",
        "        # Save best model based on validation loss\n",
        "        if avg_val_loss < best_loss:\n",
        "            best_loss = avg_val_loss\n",
        "            safe_save_model(model, 'best_diffusion_model.pt', optimizer, epoch, best_loss) # Call safe_save_model\n",
        "            print(f\"✓ New best model saved! (Val Loss: {best_loss:.4f})\")\n",
        "            no_improve_epochs = 0\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "            print(f\"No improvement for {no_improve_epochs}/{early_stopping_patience} epochs\")\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= early_stopping_patience:\n",
        "            print(\"\\nEarly stopping triggered! No improvement in validation loss.\")\n",
        "            break\n",
        "\n",
        "        # Plot loss curves every few epochs\n",
        "        if epoch % 5 == 0 or epoch == EPOCHS - 1:\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.plot(train_losses, label='Training Loss')\n",
        "            plt.plot(val_losses, label='Validation Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.title('Training and Validation Loss')\n",
        "            plt.legend()\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "# Catch errors like user interrupting (Ctrl+C)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING INTERRUPTED BY USER\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Saving current model state...\")\n",
        "    # Use avg_val_loss or last epoch loss for saving\n",
        "    last_loss = val_losses[-1] if val_losses else avg_train_loss\n",
        "    safe_save_model(model, 'interrupted_model.pt', optimizer, epoch, last_loss) # Call safe_save_model with appropriate loss\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"AN ERROR OCCURRED: {e}\")\n",
        "    print(\"=\"*50)\n",
        "    import traceback # Make sure traceback is imported\n",
        "    traceback.print_exc()\n",
        "\n",
        "finally:\n",
        "    # Final wrap-up\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Best validation loss: {best_loss:.4f}\")\n",
        "\n",
        "    # Generate final samples\n",
        "    print(\"Generating final samples...\")\n",
        "    generate_samples(model, n_samples=10) # Call generate_samples\n",
        "\n",
        "    # Display final loss curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Clean up memory\n",
        "    print(\"Cleaning up CUDA cache...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Done.\")"
      ],
      "metadata": {
        "id": "8takD04iOeST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        # Ensure out_ch is divisible by group_size, adjust if necessary\n",
        "        if out_ch % group_size != 0:\n",
        "            print(f\"Warning: GELUConvBlock out_ch ({out_ch}) not divisible by group_size ({group_size}). Adjusting group_size.\")\n",
        "            group_size = min(group_size, out_ch)\n",
        "            while out_ch % group_size != 0 and group_size > 1:\n",
        "                group_size -= 1\n",
        "            if group_size == 0:\n",
        "                 group_size = 1 # Prevent division by zero\n",
        "            print(f\"GELUConvBlock adjusted group_size to {group_size}\")\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock (kept for completeness, but DownBlock now uses Conv2d)\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, group_size):\n",
        "        super().__init__()\n",
        "        # Use named parameters (p1=2, p2=2) to fix the EinopsError\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: # Failsafe\n",
        "                valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        self.conv_block = GELUConvBlock(new_chs, new_chs, group_size)\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "# 3. HELPER CLASS: DownBlock (Corrected to use Conv2d stride 2 for downsampling)\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Downsampling block for encoding path in U-Net architecture.\n",
        "\n",
        "    This block:\n",
        "    1. Processes input features with two convolutional blocks\n",
        "    2. Downsamples spatial dimensions by 2x using a strided convolution.\n",
        "\n",
        "    Args:\n",
        "        in_chs (int): Number of input channels\n",
        "        out_chs (int): Number of output channels\n",
        "        group_size (int): Number of groups for GroupNorm\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "\n",
        "        # Ensure out_chs is divisible by group_size, adjust if necessary\n",
        "        if out_chs % group_size != 0:\n",
        "            print(f\"Warning: DownBlock out_chs ({out_chs}) not divisible by group_size ({group_size}). Adjusting group_size.\")\n",
        "            group_size = min(group_size, out_chs)\n",
        "            while out_chs % group_size != 0 and group_size > 1:\n",
        "                group_size -= 1\n",
        "            if group_size == 0:\n",
        "                 group_size = 1 # Prevent division by zero\n",
        "            print(f\"DownBlock adjusted group_size to {group_size}\")\n",
        "\n",
        "\n",
        "        # Sequential processing of features\n",
        "        layers = [\n",
        "            # First conv block changes channel dimensions\n",
        "            nn.Conv2d(in_chs, out_chs, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_chs),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # Second conv block processes features\n",
        "            nn.Conv2d(out_chs, out_chs, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_chs),\n",
        "            nn.GELU(),\n",
        "\n",
        "            # Using Conv2d with stride 2 for robust downsampling instead of RearrangePoolBlock\n",
        "            # This layer halves the spatial dimensions (H, W)\n",
        "            nn.Conv2d(out_chs, out_chs, kernel_size=4, stride=2, padding=1) # Downsampling\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "        # Log the configuration for debugging\n",
        "        print(f\"Created DownBlock: in_chs={in_chs}, out_chs={out_chs}, spatial_reduction=2x (using Conv2d stride 2)\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the DownBlock.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape [B, in_chs, H, W]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output tensor of shape [B, out_chs, H/2, W/2]\n",
        "        \"\"\"\n",
        "        return self.model(x)\n",
        "\n",
        "# 4. HELPER CLASS: UpBlock\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        # Ensure out_chs is divisible by group_size, adjust if necessary\n",
        "        # Note: The input to the *first* conv block in the sequence is 2 * in_chs\n",
        "        if out_chs % group_size != 0:\n",
        "            print(f\"Warning: UpBlock out_chs ({out_chs}) not divisible by group_size ({group_size}). Adjusting group_size for conv blocks.\")\n",
        "            group_size_conv = min(group_size, out_chs)\n",
        "            while out_chs % group_size_conv != 0 and group_size_conv > 1:\n",
        "                 group_size_conv -= 1\n",
        "            if group_size_conv == 0:\n",
        "                 group_size_conv = 1 # Prevent division by zero\n",
        "            print(f\"UpBlock adjusted conv group_size to {group_size_conv}\")\n",
        "        else:\n",
        "            group_size_conv = group_size\n",
        "\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            # First block reduces channels from 2*in_chs to out_chs\n",
        "            GELUConvBlock(2 * in_chs, out_chs, group_size_conv),\n",
        "            # Second block refines the features at the out_chs dimension\n",
        "            GELUConvBlock(out_chs, out_chs, group_size_conv)\n",
        "        )\n",
        "        print(f\"Created UpBlock: in_chs={in_chs}, out_chs={out_chs}, spatial_increase=2x\")\n",
        "\n",
        "    def forward(self, x, skip):\n",
        "        x_up = self.up(x)\n",
        "\n",
        "        # Pad x_up if its spatial dimensions are slightly smaller than skip's due to rounding\n",
        "        # This can happen with certain image sizes and padding in downsampling\n",
        "        if x_up.shape[-2:] != skip.shape[-2:]:\n",
        "             # Calculate padding amounts for height and width\n",
        "             pad_h = skip.shape[-2] - x_up.shape[-2]\n",
        "             pad_w = skip.shape[-1] - x_up.shape[-1]\n",
        "             # Apply padding (left, right, top, bottom)\n",
        "             x_up = F.pad(x_up, (0, pad_w, 0, pad_h))\n",
        "\n",
        "\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.conv(x_cat)"
      ],
      "metadata": {
        "id": "PSo1V3ouTnfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqnOWFjIMphn"
      },
      "source": [
        "## Step 6: Generating New Images\n",
        "\n",
        "Now that our model is trained, let's generate some new images! We can:\n",
        "1. Generate specific numbers\n",
        "2. Generate multiple versions of each number\n",
        "3. See how the generation process works step by step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "soKyCNcLM29y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c823fa-6323-4522-ace1-a43f266b36f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "STUDENT ACTIVITY: Try generating numbers with different noise seeds after training is complete.\n"
          ]
        }
      ],
      "source": [
        "def generate_number(model, number, n_samples=4):\n",
        "    \"\"\"\n",
        "    Generate multiple versions of a specific number using the diffusion model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The trained diffusion model\n",
        "        number (int): The digit to generate (0-9)\n",
        "        n_samples (int): Number of variations to generate\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Generated images of shape [n_samples, IMG_CH, IMG_SIZE, IMG_SIZE]\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():  # No need for gradients during generation\n",
        "        # Start with random noise\n",
        "        samples = torch.randn(n_samples, IMG_CH, IMG_SIZE, IMG_SIZE).to(device)\n",
        "\n",
        "        # Set up the number we want to generate\n",
        "        c = torch.full((n_samples,), number, dtype=torch.long).to(device) # Use integer indices (Long)\n",
        "\n",
        "        # Correctly sized conditioning mask\n",
        "        c_mask = torch.ones_like(c.unsqueeze(-1), dtype=torch.float).to(device) # Mask should be Float\n",
        "\n",
        "\n",
        "        # Display progress information\n",
        "        print(f\"Generating {n_samples} versions of number {number}...\")\n",
        "\n",
        "        # Remove noise step by step\n",
        "        for t in range(n_steps-1, -1, -1):\n",
        "            t_batch = torch.full((n_samples,), t).to(device)\n",
        "            # Pass the class indices 'c' (Long) to remove_noise, not one-hot\n",
        "            samples = remove_noise(samples, t_batch, model, c, c_mask) # Fixed: Pass 'c' instead of 'c_one_hot'\n",
        "\n",
        "            # Optional: Display occasional progress updates\n",
        "            if t % (n_steps // 5) == 0:\n",
        "                print(f\"  Denoising step {n_steps-1-t}/{n_steps-1} completed\")\n",
        "\n",
        "        return samples\n",
        "\n",
        "# Generate 4 versions of each number\n",
        "# Make sure you have successfully trained the model in the previous steps first!\n",
        "# plt.figure(figsize=(20, 10))\n",
        "# for i in range(10):\n",
        "#     # Generate samples for current digit\n",
        "#     samples = generate_number(model, i, n_samples=4)\n",
        "#\n",
        "#     # Display each sample\n",
        "#     for j in range(4):\n",
        "#         # Use 2 rows, 10 digits per row, 4 samples per digit\n",
        "#         # i//5 determines the row (0 or 1)\n",
        "#         # i%5 determines the position in the row (0-4)\n",
        "#         # j is the sample index within each digit (0-3)\n",
        "#         plt.subplot(5, 8, (i%5)*8 + (i//5)*4 + j + 1)\n",
        "#\n",
        "#         # Display the image correctly based on channel configuration\n",
        "#         if IMG_CH == 1:  # Grayscale\n",
        "#             plt.imshow(samples[j][0].cpu(), cmap='gray')\n",
        "#         else:  # Color image\n",
        "#             img = samples[j].permute(1, 2, 0).cpu()\n",
        "#             # Rescale from [-1, 1] to [0, 1] if needed\n",
        "#             if img.min() < 0:\n",
        "#                 img = (img + 1) / 2\n",
        "#             plt.imshow(img)\n",
        "#\n",
        "#         plt.title(f'Digit {i}')\n",
        "#         plt.axis('off')\n",
        "#\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# STUDENT ACTIVITY: Try generating the same digit with different noise seeds\n",
        "# This shows the variety of styles the model can produce\n",
        "print(\"\\nSTUDENT ACTIVITY: Try generating numbers with different noise seeds after training is complete.\")\n",
        "\n",
        "# Helper function to generate with seed\n",
        "def generate_with_seed(model, number, seed_value=42, n_samples=10): # Added model argument\n",
        "    torch.manual_seed(seed_value)\n",
        "    return generate_number(model, number, n_samples)\n",
        "\n",
        "# Pick a image and show many variations\n",
        "# Hint select a image e.g. dog  # Change this to any other in the dataset of subset you chose\n",
        "# Hint 2 use variations = generate_with_seed\n",
        "# Hint 3 use plt.figure and plt.imshow to display the variations\n",
        "\n",
        "# Example usage (uncomment after model is trained):\n",
        "# digit_to_generate = 7\n",
        "# num_variations = 10\n",
        "# print(f\"\\nGenerating {num_variations} variations of digit {digit_to_generate} with seed 42:\")\n",
        "# variations = generate_with_seed(model, digit_to_generate, seed_value=42, n_samples=num_variations)\n",
        "#\n",
        "# plt.figure(figsize=(num_variations * 2, 2)) # Adjust figure size\n",
        "# for i in range(num_variations):\n",
        "#     plt.subplot(1, num_variations, i+1)\n",
        "#     if IMG_CH == 1:\n",
        "#         plt.imshow(variations[i][0].cpu(), cmap='gray')\n",
        "#     else:\n",
        "#         img = variations[i].permute(1, 2, 0).cpu()\n",
        "#         if img.min() < 0:\n",
        "#             img = (img + 1) / 2\n",
        "#         plt.imshow(img)\n",
        "#     plt.title(f'Var {i+1}')\n",
        "#     plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHG1z0oSNi-q"
      },
      "source": [
        "## Step 7: Watching the Generation Process\n",
        "\n",
        "Let's see how our model turns random noise into clear images, step by step. This helps us understand how the diffusion process works!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from einops.layers.torch import Rearrange\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 1. HELPER CLASS: GELUConvBlock (Unchanged)\n",
        "class GELUConvBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, group_size):\n",
        "        super().__init__()\n",
        "        if out_ch % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while out_ch % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if out_ch % valid_group_size != 0: valid_group_size = 1\n",
        "            group_size = valid_group_size\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),\n",
        "            nn.GroupNorm(group_size, out_ch),\n",
        "            nn.GELU()\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 2. HELPER CLASS: RearrangePoolBlock (FIXED)\n",
        "# Now takes 'in_chs' and 'out_chs' and maps in_chs*4 -> out_chs\n",
        "class RearrangePoolBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.rearrange = Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
        "        new_chs = in_chs * 4\n",
        "\n",
        "        # Fix group_size for new_chs\n",
        "        if new_chs % group_size != 0:\n",
        "            valid_group_size = group_size\n",
        "            while new_chs % valid_group_size != 0 and valid_group_size > 1:\n",
        "                valid_group_size -= 1\n",
        "            if new_chs % valid_group_size != 0: valid_group_size = new_chs\n",
        "            group_size = valid_group_size\n",
        "\n",
        "        # This conv now correctly maps 4*in_chs -> out_chs\n",
        "        self.conv_block = GELUConvBlock(new_chs, out_chs, group_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.rearrange(x)\n",
        "        x = self.conv_block(x)\n",
        "        return x\n",
        "\n",
        "# 3. HELPER CLASS: DownBlock (FIXED)\n",
        "# Now calls the corrected RearrangePoolBlock\n",
        "class DownBlock(nn.Module):\n",
        "    def __init__(self, in_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            GELUConvBlock(in_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size),\n",
        "            # This now correctly takes 'out_chs' and outputs 'out_chs'\n",
        "            RearrangePoolBlock(out_chs, out_chs, group_size)\n",
        "        ]\n",
        "        self.model = nn.Sequential(*layers)\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# 4. HELPER CLASS: UpBlock (FIXED)\n",
        "# Now correctly handles different channels from skip connection\n",
        "class UpBlock(nn.Module):\n",
        "    # Takes in_chs (from below), skip_chs (from skip), and out_chs\n",
        "    def __init__(self, in_chs, skip_chs, out_chs, group_size):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_chs, in_chs, kernel_size=2, stride=2)\n",
        "        # Conv block now takes (in_chs + skip_chs)\n",
        "        self.conv = nn.Sequential(\n",
        "            GELUConvBlock(in_chs + skip_chs, out_chs, group_size),\n",
        "            GELUConvBlock(out_chs, out_chs, group_size)\n",
        "        )\n",
        "    def forward(self, x, skip):\n",
        "        x_up = self.up(x)\n",
        "        x_cat = torch.cat([x_up, skip], dim=1)\n",
        "        return self.conv(x_cat)\n",
        "\n",
        "# 5. MAIN UNET CLASS (FIXED)\n",
        "# Now calls the corrected UpBlock\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, img_ch, img_size, down_chs, t_embed_dim, c_embed_dim):\n",
        "        super"
      ],
      "metadata": {
        "id": "V_8PdAnD9cAh"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GlgESSfNrSZ"
      },
      "source": [
        "## Step 8: Adding CLIP Evaluation\n",
        "\n",
        "[CLIP](https://openai.com/research/clip) is a powerful AI model that can understand both images and text. We'll use it to:\n",
        "1. Evaluate how realistic our generated images are\n",
        "2. Score how well they match their intended numbers\n",
        "3. Help guide the generation process towards better quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "Hx-DcCBfCnQJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d3869b-a5db-416c-c3c4-4e22069fb7a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up CLIP (Contrastive Language-Image Pre-training) model...\n",
            "Installing CLIP dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling CLIP from GitHub repository...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Importing CLIP...\n",
            "✓ CLIP installation successful! Available models: ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
            "\n",
            "CLIP is now available for evaluating your generated images!\n"
          ]
        }
      ],
      "source": [
        "## Step 8: Adding CLIP Evaluation\n",
        "\n",
        "# CLIP (Contrastive Language-Image Pre-training) is a powerful model by OpenAI that connects text and images.\n",
        "# We'll use it to evaluate how recognizable our generated digits are by measuring how strongly\n",
        "# the CLIP model associates our generated images with text descriptions like \"an image of the digit 7\".\n",
        "\n",
        "# First, we need to install CLIP and its dependencies\n",
        "print(\"Setting up CLIP (Contrastive Language-Image Pre-training) model...\")\n",
        "\n",
        "# Track installation status\n",
        "clip_available = False\n",
        "\n",
        "try:\n",
        "    # Install dependencies first - these help CLIP process text and images\n",
        "    print(\"Installing CLIP dependencies...\")\n",
        "    !pip install -q ftfy regex tqdm\n",
        "\n",
        "    # Install CLIP from GitHub\n",
        "    print(\"Installing CLIP from GitHub repository...\")\n",
        "    !pip install -q git+https://github.com/openai/CLIP.git\n",
        "\n",
        "    # Import and verify CLIP is working\n",
        "    print(\"Importing CLIP...\")\n",
        "    import clip\n",
        "\n",
        "    # Test that CLIP is functioning\n",
        "    models = clip.available_models()\n",
        "    print(f\"✓ CLIP installation successful! Available models: {models}\")\n",
        "    clip_available = True\n",
        "\n",
        "except ImportError:\n",
        "    print(\"❌ Error importing CLIP. Installation might have failed.\")\n",
        "    print(\"Try manually running: !pip install git+https://github.com/openai/CLIP.git\")\n",
        "    print(\"If you're in a Colab notebook, try restarting the runtime after installation.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error during CLIP setup: {e}\")\n",
        "    print(\"Some CLIP functionality may not work correctly.\")\n",
        "\n",
        "# Provide guidance based on installation result\n",
        "if clip_available:\n",
        "    print(\"\\nCLIP is now available for evaluating your generated images!\")\n",
        "else:\n",
        "    print(\"\\nWARNING: CLIP installation failed. We'll skip the CLIP evaluation parts.\")\n",
        "\n",
        "# Import necessary libraries\n",
        "import functools\n",
        "import torch.nn.functional as F\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUdetcWECnQK"
      },
      "source": [
        " Below we are createing  a helper function to manage GPU memory when using CLIP. CLIP can be memory-intensive, so this will help prevent out-of-memory errors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "FNnAjasYCnQK"
      },
      "outputs": [],
      "source": [
        "# Memory management decorator to prevent GPU OOM errors\n",
        "def manage_gpu_memory(func):\n",
        "    \"\"\"\n",
        "    Decorator that ensures proper GPU memory management.\n",
        "\n",
        "    This wraps functions that might use large amounts of GPU memory,\n",
        "    making sure memory is properly freed after function execution.\n",
        "    \"\"\"\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        if torch.cuda.is_available():\n",
        "            # Clear cache before running function\n",
        "            torch.cuda.empty_cache()\n",
        "            try:\n",
        "                return func(*args, **kwargs)\n",
        "            finally:\n",
        "                # Clear cache after running function regardless of success/failure\n",
        "                torch.cuda.empty_cache()\n",
        "        return func(*args, **kwargs)\n",
        "    return wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "PtXUrCNACnQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65c90d5e-45b8-437a-f037-c8122a5c45bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 136MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Successfully loaded CLIP model: VisionTransformer\n",
            "\n",
            "Generating and evaluating number 0...\n",
            "Generating 4 versions of number 0...\n",
            "❌ Error in generation and evaluation loop: Given groups=1, weight of size [128, 64, 3, 3], expected input[4, 256, 14, 14] to have 64 channels, but got 256 channels instead\n",
            "Detailed error information:\n",
            "Clearing GPU cache...\n",
            "\n",
            "STUDENT ACTIVITY:\n",
            "Try the code below to evaluate a larger sample of a specific digit\n",
            "\n",
            "# Example: Generate and evaluate 10 examples of the digit 6\n",
            "# digit = 6\n",
            "# samples = generate_number(model, digit, n_samples=10)\n",
            "# similarities = evaluate_with_clip(samples, digit)\n",
            "#\n",
            "# # Calculate what percentage of samples CLIP considers \"good quality\"\n",
            "# # (either \"good handwritten\" or \"clear\" score exceeds \"blurry\" score)\n",
            "# good_or_clear = (similarities[:,0] + similarities[:,1] > similarities[:,2]).float().mean()\n",
            "# print(f\"CLIP recognized {good_or_clear.item()*100:.1f}% of the digits as good examples of {digit}\")\n",
            "#\n",
            "# # Display a grid of samples with their quality scores\n",
            "# plt.figure(figsize=(15, 8))\n",
            "# for i in range(len(samples)):\n",
            "#     plt.subplot(2, 5, i+1)\n",
            "#     plt.imshow(samples[i][0].cpu(), cmap='gray')\n",
            "#     quality = \"Good\" if similarities[i,0] + similarities[i,1] > similarities[i,2] else \"Poor\"\n",
            "#     plt.title(f\"Sample {i+1}: {quality}\", color='green' if quality == \"Good\" else 'red')\n",
            "#     plt.axis('off')\n",
            "# plt.tight_layout()\n",
            "# plt.show()\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3974156644.py\", line 195, in <cell line: 0>\n",
            "    samples = generate_number(model, number, n_samples=4)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3093716378.py\", line 32, in generate_number\n",
            "    samples = remove_noise(samples, t_batch, model, c, c_mask) # Fixed: Pass 'c' instead of 'c_one_hot'\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2207376581.py\", line 23, in remove_noise\n",
            "    predicted_noise = model(x_t, t, c, c_mask) # Fixed: Pass 'c' instead of 'c_one_hot' from generate_samples\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 140, in forward\n",
            "    x = down_block(x)\n",
            "        ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 58, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-2945402280.py\", line 24, in forward\n",
            "    return self.model(x)\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\", line 244, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1784, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 548, in forward\n",
            "    return self._conv_forward(input, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\", line 543, in _conv_forward\n",
            "    return F.conv2d(\n",
            "           ^^^^^^^^^\n",
            "RuntimeError: Given groups=1, weight of size [128, 64, 3, 3], expected input[4, 256, 14, 14] to have 64 channels, but got 256 channels instead\n"
          ]
        }
      ],
      "source": [
        "#==============================================================================\n",
        "# Step 8: CLIP Model Loading and Evaluation Setup\n",
        "#==============================================================================\n",
        "# CLIP (Contrastive Language-Image Pre-training) is a neural network that connects\n",
        "# vision and language. It was trained on 400 million image-text pairs to understand\n",
        "# the relationship between images and their descriptions.\n",
        "# We use it here as an \"evaluation judge\" to assess our generated images.\n",
        "\n",
        "# Load CLIP model with error handling\n",
        "try:\n",
        "    # Load the ViT-B/32 CLIP model (Vision Transformer-based)\n",
        "    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "    print(f\"✓ Successfully loaded CLIP model: {clip_model.visual.__class__.__name__}\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Failed to load CLIP model: {e}\")\n",
        "    clip_available = False\n",
        "    # Instead of raising an error, we'll continue with degraded functionality\n",
        "    print(\"CLIP evaluation will be skipped. Generated images will still be displayed but without quality scores.\")\n",
        "\n",
        "def evaluate_with_clip(images, target_number, max_batch_size=16):\n",
        "    \"\"\"\n",
        "    Use CLIP to evaluate generated images by measuring how well they match textual descriptions.\n",
        "\n",
        "    This function acts like an \"automatic critic\" for our generated digits by measuring:\n",
        "    1. How well they match the description of a handwritten digit\n",
        "    2. How clear and well-formed they appear to be\n",
        "    3. Whether they appear blurry or poorly formed\n",
        "\n",
        "    The evaluation process works by:\n",
        "    - Converting our images to a format CLIP understands\n",
        "    - Creating text prompts that describe the qualities we want to measure\n",
        "    - Computing similarity scores between images and these text descriptions\n",
        "    - Returning normalized scores (probabilities) for each quality\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of generated images [batch_size, channels, height, width]\n",
        "        target_number (int): The specific digit (0-9) the images should represent\n",
        "        max_batch_size (int): Maximum images to process at once (prevents GPU out-of-memory errors)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Similarity scores tensor of shape [batch_size, 3] with scores for:\n",
        "                     [good handwritten digit, clear digit, blurry digit]\n",
        "                     Each row sums to 1.0 (as probabilities)\n",
        "    \"\"\"\n",
        "    # If CLIP isn't available, return placeholder scores\n",
        "    if not clip_available:\n",
        "        print(\"⚠️ CLIP not available. Returning default scores.\")\n",
        "        # Equal probabilities (0.33 for each category)\n",
        "        return torch.ones(len(images), 3).to(device) / 3\n",
        "\n",
        "    try:\n",
        "        # For large batches, we process in chunks to avoid memory issues\n",
        "        # This is crucial when working with big images or many samples\n",
        "        if len(images) > max_batch_size:\n",
        "            all_similarities = []\n",
        "\n",
        "            # Process images in manageable chunks\n",
        "            for i in range(0, len(images), max_batch_size):\n",
        "                print(f\"Processing CLIP batch {i//max_batch_size + 1}/{(len(images)-1)//max_batch_size + 1}\")\n",
        "                batch = images[i:i+max_batch_size]\n",
        "\n",
        "                # Use context managers for efficiency and memory management:\n",
        "                # - torch.no_grad(): disables gradient tracking (not needed for evaluation)\n",
        "                # - torch.cuda.amp.autocast(): uses mixed precision to reduce memory usage\n",
        "                with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                    batch_similarities = _process_clip_batch(batch, target_number)\n",
        "                    all_similarities.append(batch_similarities)\n",
        "\n",
        "                # Explicitly free GPU memory between batches\n",
        "                # This helps prevent cumulative memory buildup that could cause crashes\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            # Combine results from all batches into a single tensor\n",
        "            return torch.cat(all_similarities, dim=0)\n",
        "        else:\n",
        "            # For small batches, process all at once\n",
        "            with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                return _process_clip_batch(images, target_number)\n",
        "\n",
        "    except Exception as e:\n",
        "        # If anything goes wrong, log the error but don't crash\n",
        "        print(f\"❌ Error in CLIP evaluation: {e}\")\n",
        "        print(f\"Traceback: {traceback.format_exc()}\")\n",
        "        # Return default scores so the rest of the notebook can continue\n",
        "        return torch.ones(len(images), 3).to(device) / 3\n",
        "\n",
        "def _process_clip_batch(images, target_number):\n",
        "    \"\"\"\n",
        "    Core CLIP processing function that computes similarity between images and text descriptions.\n",
        "\n",
        "    This function handles the technical details of:\n",
        "    1. Preparing relevant text prompts for evaluation\n",
        "    2. Preprocessing images to CLIP's required format\n",
        "    3. Extracting feature embeddings from both images and text\n",
        "    4. Computing similarity scores between these embeddings\n",
        "\n",
        "    The function includes advanced error handling for GPU memory issues,\n",
        "    automatically reducing batch size if out-of-memory errors occur.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of images to evaluate\n",
        "        target_number (int): The digit these images should represent\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Normalized similarity scores between images and text descriptions\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Create text descriptions (prompts) to evaluate our generated digits\n",
        "        # We check three distinct qualities:\n",
        "        # 1. If it looks like a handwritten example of the target digit\n",
        "        # 2. If it appears clear and well-formed\n",
        "        # 3. If it appears blurry or poorly formed (negative case)\n",
        "        text_inputs = torch.cat([\n",
        "            clip.tokenize(f\"A handwritten number {target_number}\"),\n",
        "            clip.tokenize(f\"A clear, well-written digit {target_number}\"),\n",
        "            clip.tokenize(f\"A blurry or unclear number\")\n",
        "        ]).to(device)\n",
        "\n",
        "        # Process images for CLIP, which requires specific formatting:\n",
        "\n",
        "        # 1. Handle different channel configurations (dataset-dependent)\n",
        "        if IMG_CH == 1:\n",
        "            # CLIP expects RGB images, so we repeat the grayscale channel 3 times\n",
        "            # For example, MNIST/Fashion-MNIST are grayscale (1-channel)\n",
        "            images_rgb = images.repeat(1, 3, 1, 1)\n",
        "        else:\n",
        "            # For RGB datasets like CIFAR-10/CelebA, we can use as-is\n",
        "            images_rgb = images\n",
        "\n",
        "        # 2. Normalize pixel values to [0,1] range if needed\n",
        "        # Different datasets may have different normalization ranges\n",
        "        if images_rgb.min() < 0:  # If normalized to [-1,1] range\n",
        "            images_rgb = (images_rgb + 1) / 2  # Convert to [0,1] range\n",
        "\n",
        "        # 3. Resize images to CLIP's expected input size (224x224 pixels)\n",
        "        # CLIP was trained on this specific resolution\n",
        "        resized_images = F.interpolate(images_rgb, size=(224, 224),\n",
        "                                      mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Extract feature embeddings from both images and text prompts\n",
        "        # These are high-dimensional vectors representing the content\n",
        "        image_features = clip_model.encode_image(resized_images)\n",
        "        text_features = clip_model.encode_text(text_inputs)\n",
        "\n",
        "        # Normalize feature vectors to unit length (for cosine similarity)\n",
        "        # This ensures we're measuring direction, not magnitude\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Calculate similarity scores between image and text features\n",
        "        # The matrix multiplication computes all pairwise dot products at once\n",
        "        # Multiplying by 100 scales to percentage-like values before applying softmax\n",
        "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        return similarity\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        # Special handling for CUDA out-of-memory errors\n",
        "        if \"out of memory\" in str(e):\n",
        "            # Free GPU memory immediately\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # If we're already at batch size 1, we can't reduce further\n",
        "            if len(images) <= 1:\n",
        "                print(\"❌ Out of memory even with batch size 1. Cannot process.\")\n",
        "                return torch.ones(len(images), 3).to(device) / 3\n",
        "\n",
        "            # Adaptive batch size reduction - recursively try with smaller batches\n",
        "            # This is an advanced technique to handle limited GPU memory gracefully\n",
        "            half_size = len(images) // 2\n",
        "            print(f\"⚠️ Out of memory. Reducing batch size to {half_size}.\")\n",
        "\n",
        "            # Process each half separately and combine results\n",
        "            # This recursive approach will keep splitting until processing succeeds\n",
        "            first_half = _process_clip_batch(images[:half_size], target_number)\n",
        "            second_half = _process_clip_batch(images[half_size:], target_number)\n",
        "\n",
        "            # Combine results from both halves\n",
        "            return torch.cat([first_half, second_half], dim=0)\n",
        "\n",
        "        # For other errors, propagate upward\n",
        "        raise e\n",
        "\n",
        "#==============================================================================\n",
        "# CLIP Evaluation - Generate and Analyze Sample Digits\n",
        "#==============================================================================\n",
        "# This section demonstrates how to use CLIP to evaluate generated digits\n",
        "# We'll generate examples of all ten digits and visualize the quality scores\n",
        "\n",
        "try:\n",
        "    for number in range(10):\n",
        "        print(f\"\\nGenerating and evaluating number {number}...\")\n",
        "\n",
        "        # Generate 4 different variations of the current digit\n",
        "        samples = generate_number(model, number, n_samples=4)\n",
        "\n",
        "        # Evaluate quality with CLIP (without tracking gradients for efficiency)\n",
        "        with torch.no_grad():\n",
        "            similarities = evaluate_with_clip(samples, number)\n",
        "\n",
        "        # Create a figure to display results\n",
        "        plt.figure(figsize=(15, 3))\n",
        "\n",
        "        # Show each sample with its CLIP quality scores\n",
        "        for i in range(4):\n",
        "            plt.subplot(1, 4, i+1)\n",
        "\n",
        "            # Display the image with appropriate formatting based on dataset type\n",
        "            if IMG_CH == 1:  # Grayscale images (MNIST, Fashion-MNIST)\n",
        "                plt.imshow(samples[i][0].cpu(), cmap='gray')\n",
        "            else:  # Color images (CIFAR-10, CelebA)\n",
        "                img = samples[i].permute(1, 2, 0).cpu()  # Change format for matplotlib\n",
        "                if img.min() < 0:  # Handle [-1,1] normalization\n",
        "                    img = (img + 1) / 2  # Convert to [0,1] range\n",
        "                plt.imshow(img)\n",
        "\n",
        "            # Extract individual quality scores for display\n",
        "            # These represent how confidently CLIP associates the image with each description\n",
        "            good_score = similarities[i][0].item() * 100  # Handwritten quality\n",
        "            clear_score = similarities[i][1].item() * 100  # Clarity quality\n",
        "            blur_score = similarities[i][2].item() * 100   # Blurriness assessment\n",
        "\n",
        "            # Color-code the title based on highest score category:\n",
        "            # - Green: if either \"good handwritten\" or \"clear\" score is highest\n",
        "            # - Red: if \"blurry\" score is highest (poor quality)\n",
        "            max_score_idx = torch.argmax(similarities[i]).item()\n",
        "            title_color = 'green' if max_score_idx < 2 else 'red'\n",
        "\n",
        "            # Show scores in the plot title\n",
        "            plt.title(f'Number {number}\\nGood: {good_score:.0f}%\\nClear: {clear_score:.0f}%\\nBlurry: {blur_score:.0f}%',\n",
        "                      color=title_color)\n",
        "            plt.axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        plt.close()  # Properly close figure to prevent memory leaks\n",
        "\n",
        "        # Clean up GPU memory after processing each number\n",
        "        # This is especially important for resource-constrained environments\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "except Exception as e:\n",
        "    # Comprehensive error handling to help students debug issues\n",
        "    print(f\"❌ Error in generation and evaluation loop: {e}\")\n",
        "    print(\"Detailed error information:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "    # Clean up resources even if we encounter an error\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Clearing GPU cache...\")\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "#==============================================================================\n",
        "# STUDENT ACTIVITY: Exploring CLIP Evaluation\n",
        "#==============================================================================\n",
        "# This section provides code templates for students to experiment with\n",
        "# evaluating larger batches of generated digits using CLIP.\n",
        "\n",
        "print(\"\\nSTUDENT ACTIVITY:\")\n",
        "print(\"Try the code below to evaluate a larger sample of a specific digit\")\n",
        "print(\"\"\"\n",
        "# Example: Generate and evaluate 10 examples of the digit 6\n",
        "# digit = 6\n",
        "# samples = generate_number(model, digit, n_samples=10)\n",
        "# similarities = evaluate_with_clip(samples, digit)\n",
        "#\n",
        "# # Calculate what percentage of samples CLIP considers \"good quality\"\n",
        "# # (either \"good handwritten\" or \"clear\" score exceeds \"blurry\" score)\n",
        "# good_or_clear = (similarities[:,0] + similarities[:,1] > similarities[:,2]).float().mean()\n",
        "# print(f\"CLIP recognized {good_or_clear.item()*100:.1f}% of the digits as good examples of {digit}\")\n",
        "#\n",
        "# # Display a grid of samples with their quality scores\n",
        "# plt.figure(figsize=(15, 8))\n",
        "# for i in range(len(samples)):\n",
        "#     plt.subplot(2, 5, i+1)\n",
        "#     plt.imshow(samples[i][0].cpu(), cmap='gray')\n",
        "#     quality = \"Good\" if similarities[i,0] + similarities[i,1] > similarities[i,2] else \"Poor\"\n",
        "#     plt.title(f\"Sample {i+1}: {quality}\", color='green' if quality == \"Good\" else 'red')\n",
        "#     plt.axis('off')\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JZaj-uKOC5A"
      },
      "source": [
        "## Assessment Questions\n",
        "\n",
        "Now that you've completed the exercise, answer these questions include explanations, observations, and your analysis\n",
        "Support your answers with specific examples from your experiments:\n",
        "\n",
        "### 1. Understanding Diffusion\n",
        "- Explain what happens during the forward diffusion process, using your own words and referencing the visualization examples from your notebook.\n",
        "\n",
        "- Why do we add noise gradually instead of all at once? How does this affect the learning process?\n",
        "\n",
        "- Look at the step-by-step visualization - at what point (approximately what percentage through the denoising process) can you first recognize the image? Does this vary by image?\n",
        "\n",
        "### 2. Model Architecture\n",
        "- Why is the U-Net architecture particularly well-suited for diffusion models? What advantages does it provide over simpler architectures?\n",
        "\n",
        "- What are skip connections and why are they important? Explain them in relations to our model\n",
        "\n",
        "- Describe in detail how our model is conditioned to generate specific images. How does the class conditioning mechanism work?\n",
        "\n",
        "### 3. Training Analysis (20 points)\n",
        "- What does the loss value tell of your model tell us?\n",
        "\n",
        "- How did the quality of  your  generated images change change throughout the training process?\n",
        "\n",
        "- Why do we need the time embedding in diffusion models? How does it help the model understand where it is in the denoising process?\n",
        "\n",
        "### 4. CLIP Evaluation (20 points)\n",
        "- What do the CLIP scores tell you about your generated images? Which  images got  the highest and lowest quality scores?\n",
        "\n",
        "- Develop a hypothesis explaining why certain images might be easier or harder for the model to generate convincingly.\n",
        "\n",
        "- How could CLIP scores be used to improve the diffusion model's generation process? Propose a specific technique.\n",
        "\n",
        "### 5. Practical Applications (20 points)\n",
        "- How could this type of model be useful in the real world?\n",
        "\n",
        "- What are the limitations of our current model?\n",
        "\n",
        "- If you were to continue developing this project, what three specific improvements would you make and why?\n",
        "\n",
        "### Bonus Challenge (Extra 20 points)\n",
        "Try one or more of these experiments:\n",
        "1. If you were to continue developing this project, what three specific improvements would you make and why?\n",
        "\n",
        "2. Modify the U-Net architecture (e.g., add more layers, increase channel dimensions) and train the model. How do these changes affect training time and generation quality?\n",
        "\n",
        "3. CLIP-Guided Selection: Generate 10 samples of each image, use CLIP to evaluate them, and select the top 3 highest-quality examples of each. Analyze patterns in what CLIP considers \"high quality.\"\n",
        "\n",
        "4. tyle Conditioning: Modify the conditioning mechanism to generate multiple styles of the same digit (e.g., slanted, thick, thin). Document your approach and results.\n",
        "\n",
        "\n",
        "Deliverables:\n",
        "1. A PDF copy of your notebook with\n",
        "    - Complete code, outputs, and generated images\n",
        "    - Include all experiment results, training plots, and generated samples\n",
        "    - CLIP evaluation scores of ythe images you generated\n",
        "    - Answers and any interesting findings from the bonus challenges"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}